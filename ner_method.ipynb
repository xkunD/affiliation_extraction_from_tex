{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('\\\\author{\\n    Danielle C. Maddix \\\\\\\\ Amazon Research \\\\\\\\ 2795 Augustine Dr. \\\\\\\\ Santa Clara, CA 95054 \\\\\\\\ \\texttt{dmmaddix@amazon.com} \\\\\\\\\\n    \\\\And\\n    Nadim Saad \\\\\\\\ Stanford University \\\\\\\\ 450 Serra Mall \\\\\\\\ Stanford, CA 94305 \\\\\\\\ \\texttt{nsaad31@stanford.edu} \\\\\\\\\\n    \\\\And\\n    Yuyang Wang \\\\\\\\ Amazon Research \\\\\\\\ 2795 Augustine Dr. \\\\\\\\ Santa Clara, CA 95054 \\\\\\\\ \\texttt{yuyawang@amazon.com} \\\\\\\\\\n    }', {'entities': [(35, 50, 'INSTITUTION'), (159, 178, 'INSTITUTION')]})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-20 22:07:14.794917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'nlp' has no attribute 'to_disk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Save the trained model to disk\u001b[39;00m\n\u001b[1;32m     43\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./trained_ner_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_disk\u001b[49m(output_dir)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'nlp' has no attribute 'to_disk'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Modified function to handle multiple institutions\n",
    "def create_training_data_multiple_affiliations(templates, institutions):\n",
    "    training_data = []\n",
    "    \n",
    "    for template in templates:\n",
    "        text = template\n",
    "        entities = []\n",
    "        \n",
    "        # For each institution, find its position in the text\n",
    "        for institution in institutions:\n",
    "            entity_start = text.find(institution)\n",
    "            if entity_start != -1:  # Only include if found\n",
    "                entity_end = entity_start + len(institution)\n",
    "                entities.append((entity_start, entity_end, \"INSTITUTION\"))\n",
    "        \n",
    "        # Append the text and entity annotations if at least one institution was found\n",
    "        if entities:\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Example usage\n",
    "templates = [\n",
    "    \"\"\"\\\\author{\n",
    "    Danielle C. Maddix \\\\\\\\ Amazon Research \\\\\\\\ 2795 Augustine Dr. \\\\\\\\ Santa Clara, CA 95054 \\\\\\\\ \\texttt{dmmaddix@amazon.com} \\\\\\\\\n",
    "    \\And\n",
    "    Nadim Saad \\\\\\\\ Stanford University \\\\\\\\ 450 Serra Mall \\\\\\\\ Stanford, CA 94305 \\\\\\\\ \\texttt{nsaad31@stanford.edu} \\\\\\\\\n",
    "    \\And\n",
    "    Yuyang Wang \\\\\\\\ Amazon Research \\\\\\\\ 2795 Augustine Dr. \\\\\\\\ Santa Clara, CA 95054 \\\\\\\\ \\texttt{yuyawang@amazon.com} \\\\\\\\\n",
    "    }\"\"\"\n",
    "]\n",
    "\n",
    "institutions = [\"Amazon Research\", \"Stanford University\"]\n",
    "\n",
    "# Generate training data\n",
    "training_data = create_training_data_multiple_affiliations(templates, institutions)\n",
    "print(training_data)\n",
    "\n",
    "import nlp\n",
    "# Save the trained model to disk\n",
    "output_dir = \"./trained_ner_model\"\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "def match_to_ror(extracted_affiliation, ror_institutions):\n",
    "    best_match, score = process.extractOne(extracted_affiliation, ror_institutions)\n",
    "    return best_match if score > 80 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to extract institution names and aliases from ROR data\n",
    "def extract_institution_names(ror_data):\n",
    "    institution_names = []\n",
    "    for entry in ror_data:\n",
    "        for name_entry in entry['names']:\n",
    "            institution_names.append(name_entry['value'])\n",
    "    return institution_names\n",
    "\n",
    "# Load the ROR dataset from JSON\n",
    "with open(\"ror.json\", \"r\") as f:\n",
    "    ror_data = json.load(f)\n",
    "\n",
    "# Extract all institution names\n",
    "institution_names = extract_institution_names(ror_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_training_data() missing 1 required positional argument: 'templates'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m training_data\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create training data using the ROR institution names\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m TRAIN_DATA \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstitution_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_training_data() missing 1 required positional argument: 'templates'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def create_training_data(institution_names, templates):\n",
    "\n",
    "    \n",
    "    training_data = []\n",
    "    for institution in institution_names:\n",
    "        template = random.choice(templates)\n",
    "        text = template.format(institution)\n",
    "        entity_start = text.find(institution)\n",
    "        entity_end = entity_start + len(institution)\n",
    "        \n",
    "        training_data.append((text, {\"entities\": [(entity_start, entity_end, \"INSTITUTION\")]}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Create training data using the ROR institution names\n",
    "TRAIN_DATA = create_training_data(institution_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Load the ROR JSON database\n",
    "with open(\"ror.json\", \"r\") as f:\n",
    "    ror_data = json.load(f)\n",
    "\n",
    "# Extract institution names from ROR data\n",
    "def extract_ror_institutions(ror_data):\n",
    "    institution_names = []\n",
    "    for entry in ror_data:\n",
    "        for name in entry['names']:\n",
    "            institution_names.append(name['value'])\n",
    "    return institution_names\n",
    "\n",
    "ror_institution_names = extract_ror_institutions(ror_data)\n",
    "\n",
    "# Function to extract affiliations from LaTeX\n",
    "def extract_affiliations_from_latex(latex_text):\n",
    "    affiliation_pattern = r'\\\\affiliation\\{(.*?)\\}'\n",
    "    affiliations = re.findall(affiliation_pattern, latex_text, re.DOTALL)\n",
    "    cleaned_affiliations = [affil.replace('\\n', ' ').strip() for affil in affiliations]\n",
    "    return cleaned_affiliations\n",
    "\n",
    "# Match extracted affiliation to ROR institution names using fuzzy matching\n",
    "def match_to_ror(extracted_affiliation, ror_institutions):\n",
    "    best_match, score = process.extractOne(extracted_affiliation, ror_institutions)\n",
    "    return best_match if score > 80 else None\n",
    "\n",
    "# Generate training data using LaTeX and ROR names\n",
    "def create_training_data_from_latex_ror(latex_text, ror_institution_names):\n",
    "    affiliations = extract_affiliations_from_latex(latex_text)\n",
    "    training_data = []\n",
    "    \n",
    "    for affiliation in affiliations:\n",
    "        matched_ror = match_to_ror(affiliation, ror_institution_names)\n",
    "        if matched_ror:\n",
    "            entity_start = latex_text.find(affiliation)\n",
    "            entity_end = entity_start + len(affiliation)\n",
    "            training_data.append((latex_text, {\"entities\": [(entity_start, entity_end, \"INSTITUTION\")]}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Example LaTeX text\n",
    "latex_text = r'''\n",
    "\\author{Giorgio Busoni}\n",
    " \\email{giorgio.busoni@mpi-hd.mpg.de}\n",
    "\\affiliation{\n",
    " Max-Planck-Institut fur Kernphysik,\\\\ Saupfercheckweg 1, 69117 Heidelberg, Germany\n",
    "}\n",
    "'''\n",
    "\n",
    "# Generate training data\n",
    "training_data = create_training_data_from_latex_ror(latex_text, ror_institution_names)\n",
    "print(\"Generated Training Data:\", training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.20\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "\n",
    "# Load ROR dataset\n",
    "with open('ror.json', 'r') as f:\n",
    "    ror_data = json.load(f)\n",
    "\n",
    "# Extract institution names\n",
    "def extract_institution_names(ror_data):\n",
    "    institution_names = []\n",
    "    for entry in ror_data:\n",
    "        for name_entry in entry['names']:\n",
    "            institution_names.append(name_entry['value'])\n",
    "    return institution_names\n",
    "\n",
    "institution_names = extract_institution_names(ror_data)\n",
    "\n",
    "# Create training examples from the ROR institution names\n",
    "def create_training_data_from_ror(institution_names, num_samples=1000):\n",
    "    templates = [\n",
    "        \"{} is a leading research institute.\",\n",
    "        \"The work was done at {}.\",\n",
    "        \"The collaboration between {} and other universities is remarkable.\",\n",
    "        \"{} has been a pioneer in the field.\"\n",
    "    ]\n",
    "    \n",
    "    training_data = []\n",
    "    for _ in range(num_samples):\n",
    "        institution = random.choice(institution_names)\n",
    "        template = random.choice(templates)\n",
    "        text = template.format(institution)\n",
    "        entity_start = text.find(institution)\n",
    "        entity_end = entity_start + len(institution)\n",
    "        training_data.append((text, {\"entities\": [(entity_start, entity_end, \"INSTITUTION\")]}))\n",
    "    \n",
    "    return training_data\n",
    "\n",
    "# Create training data\n",
    "TRAIN_DATA = create_training_data_from_ror(institution_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at Centurion Ilac San. ve Tic A....\" with entities \"[(21, 52, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at Clinical Tools, Inc..\" with entities \"[(21, 41, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at IfM.\" with entities \"[(21, 24, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Losses: {'ner': 1037.5942916136526}\n",
      "Iteration 1 - Losses: {'ner': 160.33088823126627}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Train the NER model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrain_ner_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_DATA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 18\u001b[0m, in \u001b[0;36mtrain_ner_model\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m     16\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mmake_doc(text)\n\u001b[1;32m     17\u001b[0m     example \u001b[38;5;241m=\u001b[39m Example\u001b[38;5;241m.\u001b[39mfrom_dict(doc, annotations)\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/language.py:1204\u001b[0m, in \u001b[0;36mLanguage.update\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1199\u001b[0m         name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty\u001b[38;5;241m.\u001b[39mTrainableComponent)\n\u001b[1;32m   1201\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_trainable\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1203\u001b[0m     ):\n\u001b[0;32m-> 1204\u001b[0m         \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msgd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m annotates:\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc, eg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m   1207\u001b[0m         _pipe(\n\u001b[1;32m   1208\u001b[0m             (eg\u001b[38;5;241m.\u001b[39mpredicted \u001b[38;5;28;01mfor\u001b[39;00m eg \u001b[38;5;129;01min\u001b[39;00m examples),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1214\u001b[0m         examples,\n\u001b[1;32m   1215\u001b[0m     ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:252\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.finish_update\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/thinc/model.py:346\u001b[0m, in \u001b[0;36mModel.finish_update\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39mparam_names:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mhas_grad(name):\n\u001b[0;32m--> 346\u001b[0m         param, grad \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    349\u001b[0m         node\u001b[38;5;241m.\u001b[39mset_param(name, param)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/thinc/optimizers.py:234\u001b[0m, in \u001b[0;36mOptimizer.__call__\u001b[0;34m(self, key, weights, gradient, lr_scale)\u001b[0m\n\u001b[1;32m    230\u001b[0m     weights, gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_radam(\n\u001b[1;32m    231\u001b[0m         ops, weights, gradient, lr_scale, key, nr_upd\n\u001b[1;32m    232\u001b[0m     )\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb1 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m--> 234\u001b[0m     weights, gradient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_adam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_upd\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m  \u001b[38;5;66;03m# TODO: error message\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/thinc/optimizers.py:344\u001b[0m, in \u001b[0;36mOptimizer._adam\u001b[0;34m(self, ops, weights, gradient, lr_scale, key, nr_upd)\u001b[0m\n\u001b[1;32m    342\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps\n\u001b[1;32m    343\u001b[0m \u001b[38;5;66;03m# needs to be 1D going into the adam function\u001b[39;00m\n\u001b[0;32m--> 344\u001b[0m weights_1D, gradient_1D, mom1, mom2 \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_1D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmom1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmom2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr_scale\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmom1[key] \u001b[38;5;241m=\u001b[39m mom1\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmom2[key] \u001b[38;5;241m=\u001b[39m mom2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create blank spaCy model\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add the \"INSTITUTION\" label\n",
    "ner.add_label(\"INSTITUTION\")\n",
    "\n",
    "# Prepare the training data for spaCy\n",
    "def train_ner_model(train_data):\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    for itn in range(10):  # 10 iterations of training\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        for text, annotations in train_data:\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            nlp.update([example], drop=0.5, losses=losses)\n",
    "        print(f\"Iteration {itn} - Losses: {losses}\")\n",
    "\n",
    "# Train the NER model\n",
    "train_ner_model(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Amazon Research and Stanford University resulted in significant advancements, Label: INSTITUTION\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to test the model on unseen data\n",
    "def test_ner_model(test_data, model):\n",
    "    correct_predictions = 0\n",
    "    total_entities = 0\n",
    "    all_entities = []\n",
    "\n",
    "    for text, annotations in test_data:\n",
    "        doc = model(text)\n",
    "        predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        true_entities = annotations['entities']\n",
    "\n",
    "        # Check if the predicted entities match the true entities\n",
    "        for pred_entity in predicted_entities:\n",
    "            if any(text[start:end] == pred_entity[0] for start, end, label in true_entities):\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        total_entities += len(true_entities)\n",
    "        all_entities.append((predicted_entities, true_entities))\n",
    "\n",
    "    accuracy = correct_predictions / total_entities if total_entities else 0\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return all_entities\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_ner_model(test_data, nlp)\n",
    "\n",
    "test_paragraph = \"\"\"\n",
    "The collaboration between Amazon Research and Stanford University resulted in significant advancements.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(test_paragraph)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at Clinical Tools, Inc..\" with entities \"[(21, 41, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at IfM.\" with entities \"[(21, 24, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"The work was done at Centurion Ilac San. ve Tic A....\" with entities \"[(21, 52, 'INSTITUTION')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Losses: {'ner': 10.966555260701712}\n",
      "Iteration 1, Losses: {'ner': 27.075484868097707}\n",
      "Iteration 2, Losses: {'ner': 15.901708882065583}\n",
      "Iteration 3, Losses: {'ner': 27.467168395733385}\n",
      "Iteration 4, Losses: {'ner': 13.9366148383876}\n",
      "Iteration 5, Losses: {'ner': 5.31615356069249}\n",
      "Iteration 6, Losses: {'ner': 30.795950481227877}\n",
      "Iteration 7, Losses: {'ner': 9.84563931631919}\n",
      "Iteration 8, Losses: {'ner': 10.236311512922503}\n",
      "Iteration 9, Losses: {'ner': 9.498716846197235}\n"
     ]
    }
   ],
   "source": [
    "ner = nlp.get_pipe(\"ner\")\n",
    "def fine_tune_ner_model(train_data):\n",
    "    # Add the \"ORG\" label if not already present\n",
    "    ner.add_label(\"ORG\")\n",
    "    \n",
    "    # Disable other pipelines during training, except 'ner'\n",
    "    unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    # Begin training\n",
    "    with nlp.disable_pipes(*unaffected_pipes):  # only train NER\n",
    "        optimizer = nlp.resume_training()\n",
    "\n",
    "        for itn in range(10):  # 10 training iterations\n",
    "            random.shuffle(train_data)\n",
    "            losses = {}\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], drop=0.5, losses=losses)\n",
    "            print(f\"Iteration {itn}, Losses: {losses}\")\n",
    "\n",
    "# Fine-tune the pre-trained model\n",
    "fine_tune_ner_model(TRAIN_DATA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n",
      "Entity: Amazon Research and Stanford University, Label: INSTITUTION\n",
      "Entity: .\n",
      ", Label: INSTITUTION\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "nlp.to_disk(\"./fine_tuned_ner_model\")\n",
    "print(\"Model saved!\")\n",
    "\n",
    "# Load the model for testing\n",
    "nlp_fine_tuned = spacy.load(\"./fine_tuned_ner_model\")\n",
    "\n",
    "# Test on a paragraph\n",
    "test_paragraph = \"\"\"\n",
    "The collaboration between Amazon Research and Stanford University resulted in significant advancements.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp_fine_tuned(test_paragraph)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(TRAIN_DATA, test_size=0.2, random_state=42)\n",
    "\n",
    "# Evaluate on the test data\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "for text, annotations in test_data:\n",
    "    doc = nlp_fine_tuned(text)\n",
    "    predicted_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    true_entities = [(text[start:end], label) for start, end, label in annotations[\"entities\"]]\n",
    "    \n",
    "    for pred_entity in predicted_entities:\n",
    "        if pred_entity in true_entities:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    total_predictions += len(true_entities)\n",
    "\n",
    "accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
