{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "import zipfile\n",
    "import importlib\n",
    "import regex as re\n",
    "import pyperclip  \n",
    "import TexSoup as TS\n",
    "from TexSoup.tokens import MATH_ENV_NAMES\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unmatched 'Primary Org Id' values: 24\n",
      "Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\n",
      "Total ArXiv IDs: 931\n",
      "Found text files: 931\n",
      "Percentage: 100.00%\n",
      "\n",
      "Missing files:\n",
      "Preprocessed data saved to data/preprocessed_scopus_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "scopus_data = pd.read_csv('data/2201.00_scopus_931.csv')\n",
    "ror_mapping = pd.read_csv('matching_data/matched_results_fuzzy.csv')\n",
    "\n",
    "# Merge the datasets on 'Primary Org Id'\n",
    "merged_data = pd.merge(\n",
    "    scopus_data,\n",
    "    ror_mapping,\n",
    "    on='Primary Org Id',\n",
    "    how='left'  # Use 'left' to keep all rows from scopus_data even if no match is found\n",
    ")\n",
    "\n",
    "# Calculate the number of unmatched rows\n",
    "unmatched_count = merged_data['ROR ID'].isna().sum()\n",
    "\n",
    "print(f\"Number of unmatched 'Primary Org Id' values: {unmatched_count}\")\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('data/2201.00_scopus_931_with_ror.csv', index=False)\n",
    "\n",
    "print(\"Matching completed. The output is saved as 'data/2201.00_scopus_931_with_ror.csv'.\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "csv_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "text_folder = 'data/2201_00_text'\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Assuming the column containing ArXiv IDs is named 'ArXiv Id'\n",
    "arxiv_ids = df['ArXiv Id'].dropna().unique()  # Drop NaN values and get unique IDs\n",
    "\n",
    "# Initialize counters and a list to store missing files\n",
    "found_count = 0\n",
    "total_count = len(arxiv_ids)\n",
    "missing_files = []\n",
    "\n",
    "# Check for each ArXiv ID\n",
    "for arxiv_id in arxiv_ids:\n",
    "    txt_file_path = os.path.join(text_folder, f\"{arxiv_id}.txt\")\n",
    "    if os.path.isfile(txt_file_path):\n",
    "        found_count += 1\n",
    "    else:\n",
    "        missing_files.append(arxiv_id)\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage = (found_count / total_count) * 100\n",
    "\n",
    "# Display results\n",
    "print(f\"Total ArXiv IDs: {total_count}\")\n",
    "print(f\"Found text files: {found_count}\")\n",
    "print(f\"Percentage: {percentage:.2f}%\")\n",
    "print(\"\\nMissing files:\")\n",
    "for missing_id in missing_files:\n",
    "    print(missing_id)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'data/2201.00_scopus_931_with_ror.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select the desired columns\n",
    "columns_to_keep = ['Primary Org Id', 'Primary Org Name_x', 'ArXiv Id', 'ROR ID']\n",
    "df_selected = df[columns_to_keep]\n",
    "\n",
    "# Save the preprocessed data to a new CSV file\n",
    "output_file_path = 'data/preprocessed_scopus_data.csv'\n",
    "df_selected.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = \"data/2201.00_scopus_931_with_ror.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Initialize the TrieExtractor\n",
    "extractor = TrieExtractor(data_path=\"data/1.34_extracted_ror_data.csv\", common_words_path=\"data/common_english_words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import re\n",
    "import io\n",
    "\n",
    "def find_doc_class(fp, name_match=False):\n",
    "    \"\"\"Search for document class related lines in a file and return a code to represent the type\"\"\"\n",
    "    doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "    sub_doc_class = re.compile(r\"^\\s*\\\\document(?:style|class).*(?:\\{standalone\\}|\\{subfiles\\})\")\n",
    "\n",
    "    file_content = fp.read()\n",
    "    try:\n",
    "        file_text = file_content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        file_text = file_content.decode('latin-1')\n",
    "\n",
    "    for line in file_text.splitlines():\n",
    "        if doc_class_pat.search(line):\n",
    "            if name_match:\n",
    "                if sub_doc_class.search(line):\n",
    "                    return -99999\n",
    "                return 1  # Found document class line\n",
    "    return 0  # No document class line found\n",
    "\n",
    "def find_main_tex_source_in_folder(zip_file, folder_name):\n",
    "    \"\"\"Find the main .tex file inside a folder within the zip archive\"\"\"\n",
    "    tex_names = {\"paper\", \"main\", \"ms.\", \"article\"}\n",
    "    \n",
    "    # Get all .tex files in the folder\n",
    "    tex_files = [f for f in zip_file.namelist() if f.startswith(folder_name + '/') and f.endswith('.tex')]\n",
    "\n",
    "    if len(tex_files) == 1:\n",
    "        return tex_files[0]  # If there's only one .tex file, return it\n",
    "\n",
    "    main_files = {}\n",
    "    for tex_file in tex_files:\n",
    "        depth = tex_file.count('/') - folder_name.count('/')  # Depth relative to folder\n",
    "        has_main_name = any(kw in tex_file for kw in tex_names)\n",
    "        \n",
    "        with zip_file.open(tex_file) as fp:\n",
    "            main_files[tex_file] = find_doc_class(fp, name_match=has_main_name) - depth\n",
    "\n",
    "    return max(main_files, key=main_files.get) if main_files else None\n",
    "\n",
    "def pre_format(text):\n",
    "    \"\"\"Format LaTeX text by adding spaces where necessary\"\"\"\n",
    "    # return text.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }').replace(')$', ') $')\n",
    "    return text\n",
    "\n",
    "def source_from_zip(zip_file, folder_name):\n",
    "    \"\"\"Extract and decode the main .tex file from a folder inside the zip archive\"\"\"\n",
    "    tex_main = find_main_tex_source_in_folder(zip_file, folder_name)\n",
    "    if tex_main:\n",
    "        with zip_file.open(tex_main) as fp:\n",
    "            file_content = fp.read()\n",
    "            try:\n",
    "                source_text = pre_format(file_content.decode('utf-8'))\n",
    "            except UnicodeDecodeError:\n",
    "                source_text = pre_format(file_content.decode('latin-1'))\n",
    "            return source_text\n",
    "    return None\n",
    "\n",
    "def extract_before_abstract(source_text):\n",
    "    \"\"\"Extract text before the abstract section\"\"\"\n",
    "    no_comments_text = re.sub(r'(?<!\\\\)%.*', '', source_text)  # Remove comments\n",
    "    # no_usepackage_text = re.sub(r'\\\\usepackage\\s*\\{[^}]+\\}', '', no_comments_text)  # Remove usepackage\n",
    "    # text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', no_usepackage_text)  # Remove LaTeX commands\n",
    "    text = no_comments_text\n",
    "    # text = re.sub(r'\\\\[a-zA-Z]+\\[[^\\]]*\\]\\{[^}]*\\}', '', text)\n",
    "    # text = re.sub(r'\\$[^$]*\\$', '', text)  # Remove inline math\n",
    "    # text = text.replace('{', '').replace('}', '').replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # output_file.write(f\"text: {text}\\n\")\n",
    "\n",
    "    abstract_word_match = re.search(r'begin{abstract}', text, re.IGNORECASE)\n",
    "    if abstract_word_match:\n",
    "        return text[:abstract_word_match.start()].strip()\n",
    "\n",
    "    abstract_match = re.search(r'abstract{', text)\n",
    "    if abstract_match:\n",
    "        return text[:abstract_match.start()].strip()\n",
    "    \n",
    "    nomacro_abstract_match_2 = re.search(r'abstract', text)\n",
    "    if nomacro_abstract_match_2:\n",
    "        return text[:nomacro_abstract_match_2.start()].strip()\n",
    "    \n",
    "    nomacro_abstract_match_1 = re.search(r'Abstract', text)\n",
    "    if nomacro_abstract_match_1:\n",
    "        return text[:nomacro_abstract_match_1.start()].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "zip_file_path = \"./2311_tex_test.zip\"\n",
    "output_path = \"./output.txt\"\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    base_folder = zip_file.namelist()[0]\n",
    "    folders = {name.rstrip('/') for name in zip_file.namelist() if name.startswith(base_folder) and name.endswith('/') and name != base_folder}  # Get folder names\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for folder in folders:\n",
    "            source_text = source_from_zip(zip_file, folder)\n",
    "            # output_file.write(f\"{folder}:source text: {source_text}\\n\")\n",
    "            if source_text:\n",
    "                content_before_abstract = extract_before_abstract(source_text)\n",
    "                if content_before_abstract:\n",
    "                    output_file.write(f\"Content before abstract in {folder}:\\n{content_before_abstract}\\n\\n\")\n",
    "                else:\n",
    "                    output_file.write(f\"No abstract found in {folder}, or no content before abstract.\\n\\n\")\n",
    "            else:\n",
    "                output_file.write(f\"No source found in {folder}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 83\n",
      "Total number of files with valid content before abstract: 80\n",
      "Filtered output saved in: filtered_files_with_content_macro.txt\n"
     ]
    }
   ],
   "source": [
    "# Read input from a text file and filter out files without valid content before the abstract\n",
    "input_file_path = 'output.txt'  # Replace with your actual file path\n",
    "output_file_path = 'filtered_files_with_content_macro.txt'\n",
    "\n",
    "# Variables to keep track of statistics\n",
    "total_files_count_author = 0\n",
    "valid_files_count = 0\n",
    "valid_files_with_content = []\n",
    "\n",
    "# Reading and processing the input file\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "    \n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count_author += 1\n",
    "            lines = block.split('\\n', 1)  # Split to separate the file name from its content\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                \n",
    "                # Check if the content does not indicate \"No abstract found\"\n",
    "                if 'No abstract found' not in content and 'no content before abstract' not in content.lower():\n",
    "                    valid_files_count += 1\n",
    "                    valid_files_with_content.append((file_name, content))\n",
    "\n",
    "# Writing the filtered results to an output file\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for file_name, content in valid_files_with_content:\n",
    "        outfile.write(f\"Content before abstract in {file_name}:\\n{content}\\n\\n\")\n",
    "\n",
    "# Print or save the statistics summary\n",
    "print(f\"Total number of files processed: {total_files_count_author}\")\n",
    "print(f\"Total number of files with valid content before abstract: {valid_files_count}\")\n",
    "print(f\"Filtered output saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 80\n",
      "Extracted tag data saved in: ./tagged_outputs/all_extracted_tags_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json  # For structured storage\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = 'filtered_files_with_content_macro.txt'\n",
    "output_file_path = './tagged_outputs/all_extracted_tags_cleaned.txt'  # Single output file\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Tags to search for (without leading backslashes for dictionary keys)\n",
    "tags_to_search = [\n",
    "    'institution', 'affiliations', 'affiliation', 'icmlaffiliation',\n",
    "    'institute', 'affil', 'aff', 'AFF', 'university', 'address'\n",
    "]\n",
    "\n",
    "# Dictionary to store extracted data\n",
    "extracted_data = {}\n",
    "\n",
    "# Function to extract content inside the first level of braces `{}` (handling optional `[]`)\n",
    "def extract_tag_content(tag, content):\n",
    "    pattern = rf\"(\\\\{tag})(\\[[^\\]]*\\])?\\{{\"\n",
    "    results = []\n",
    "    start = 0\n",
    "\n",
    "    while (match := re.search(pattern, content[start:])) is not None:\n",
    "        start_idx = start + match.end()  # Start after the macro (past `{`)\n",
    "        brace_level = 1\n",
    "        end_idx = start_idx\n",
    "\n",
    "        # Find the matching closing brace\n",
    "        while brace_level > 0 and end_idx < len(content):\n",
    "            if content[end_idx] == '{':\n",
    "                brace_level += 1\n",
    "            elif content[end_idx] == '}':\n",
    "                brace_level -= 1\n",
    "            end_idx += 1\n",
    "\n",
    "        # Extract only the content inside the first `{}` (excluding the macro name)\n",
    "        extracted_content = content[start_idx:end_idx - 1].strip()  # Remove trailing `}`\n",
    "        results.append(extracted_content)\n",
    "        start = end_idx  # Move to next occurrence\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and store extracted content\n",
    "def extract_and_store_tags(file_name, content, tags):\n",
    "    extracted_data[file_name] = {}  # Initialize storage for this file\n",
    "    found_any_tag = False  # Flag to check if we found any tag\n",
    "\n",
    "    for tag in tags:\n",
    "        matches = extract_tag_content(tag, content)\n",
    "\n",
    "        if matches:\n",
    "            extracted_data[file_name][tag] = matches  # Store under cleaned key\n",
    "            found_any_tag = True  # At least one macro was found\n",
    "\n",
    "    # If no macros were found, store the whole pre-abstract content\n",
    "    if not found_any_tag:\n",
    "        extracted_data[file_name][\"full_content\"] = content\n",
    "\n",
    "# Process input file\n",
    "total_files_count = 0\n",
    "\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():\n",
    "            total_files_count += 1\n",
    "            lines = block.split('\\n', 1)\n",
    "\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                extract_and_store_tags(file_name, content, tags_to_search)\n",
    "\n",
    "# Save extracted data to a single output file (JSON format for easy reuse)\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(extracted_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total number of files processed: {total_files_count}\")\n",
    "print(f\"Extracted tag data saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:No persistent OpenAlex cache found. Using empty cache.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# OpenAlex API Information\n",
    "OPENALEX_SEARCH_URL = 'https://api.openalex.org/institutions'\n",
    "\n",
    "# Initialize cache\n",
    "openalex_cache = {}\n",
    "\n",
    "# Load persistent cache\n",
    "cache_file = 'openalex_cache.json'\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        openalex_cache = json.load(f)\n",
    "    logging.info(\"Loaded persistent OpenAlex cache.\")\n",
    "else:\n",
    "    logging.info(\"No persistent OpenAlex cache found. Using empty cache.\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
    "def query_openalex(institution_name):\n",
    "    \"\"\"\n",
    "    Query OpenAlex API for institution details.\n",
    "    \"\"\"\n",
    "    if institution_name in openalex_cache:\n",
    "        return openalex_cache[institution_name]\n",
    "    \n",
    "    params = {'search': institution_name}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(OPENALEX_SEARCH_URL, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('results'):\n",
    "                first_result = data['results'][0]  # Get the first search result\n",
    "                \n",
    "                openalex_cache[institution_name] = {\n",
    "                    'OpenAlex_ID': first_result.get('id', 'N/A'),\n",
    "                    'Name': first_result.get('display_name', 'N/A'),\n",
    "                    'Hint': first_result.get('hint', 'N/A'),\n",
    "                    'Cited_by_Count': first_result.get('cited_by_count', 'N/A'),\n",
    "                    'Works_Count': first_result.get('works_count', 'N/A'),\n",
    "                    'External_ID': first_result.get('external_id', 'N/A')  # This may include a ROR link\n",
    "                }\n",
    "                logging.info(f\"First OpenAlex result found for '{institution_name}': {first_result.get('id', 'N/A')}\")\n",
    "                return openalex_cache[institution_name]\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"No OpenAlex information found for '{institution_name}'.\")\n",
    "                openalex_cache[institution_name] = None\n",
    "                return None\n",
    "        else:\n",
    "            logging.error(f\"OpenAlex API query failed with status code: {response.status_code} for institution: {institution_name}\")\n",
    "            openalex_cache[institution_name] = None\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"OpenAlex API query exception for '{institution_name}', Error: {e}\")\n",
    "        openalex_cache[institution_name] = None\n",
    "        return None\n",
    "\n",
    "# Save cache to a file after all queries\n",
    "def save_cache():\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(openalex_cache, f)\n",
    "    logging.info(\"Saved OpenAlex cache to file.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = 'tagged_outputs/all_extracted_tags_cleaned.txt'\n",
    "output_file_path = 'institution_output_with_ror.json'\n",
    "\n",
    "# Load JSON data\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "    extracted_data = json.load(infile)\n",
    "\n",
    "# Load TrieExtractor for full content/address processing\n",
    "extractor = TrieExtractor(\n",
    "    data_path=\"data/1.34_extracted_ror_data.csv\",\n",
    "    common_words_path=\"data/common_english_words.txt\"\n",
    ")\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No OpenAlex information found for 'The Oskar Klein Centre, Department of Physics, Stockholm University, Stockholm 106 91, Sweden'.\n",
      "WARNING:root:No OpenAlex information found for 'Dipartimento Interateneo di Fisica ``Michelangelo Merlin,'' Via Amendola 173, 70126 Bari, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'Istituto Nazionale di Fisica Nucleare - Sezione di Bari, Via Orabona 4, 70126 Bari, Italy'.\n",
      "ERROR:root:OpenAlex API query failed with status code: 500 for institution: Institut f\\\"{u}r Theoretische Physik, Universit\\\"{a}t Heidelberg, Philosophenweg 16, 69120, Heidelberg, Germany\n",
      "ERROR:root:OpenAlex API query failed with status code: 500 for institution: Universit\\\"{a}t Heidelberg, Kirchhoff-Institut f\\\"{u}r Physik, Im Neuenheimer Feld 227, 69120 Heidelberg, Germany\n",
      "WARNING:root:No OpenAlex information found for 'Dipartimento di Fisica ``E.R. Caianiello'', Università degli Studi di Salerno, Via Giovanni Paolo II, 132 - 84084 Fisciano (SA), Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'INFN - Gruppo Collegato di Salerno, Via Giovanni Paolo II, 132 - 84084 Fisciano (SA), Italy.'.\n",
      "WARNING:root:No OpenAlex information found for 'LAPTh, CNRS, USMB, F-74000 Annecy, France.'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Physics and Astronomy, University of Minnesota, Minneapolis, MN 55455, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Loughborough University, Loughborough, LE11 3TU, UK'.\n",
      "WARNING:root:No OpenAlex information found for 'Technical University of Munich, Germany; Institute for Advanced Study, D-85748 Garching, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, and Stewart Blusson Quantum Matter Institute, University of British Columbia, Vancouver, BC, Canada V6T 1Z1'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Theoretical Physics, Wroclaw University of Science and Technology, 50-370 Wroclaw, Poland'.\n",
      "WARNING:root:No OpenAlex information found for 'Facultad de Ciencias, Universidad Nacional Aut\\'onoma de M\\'exico, Investigaci\\'on Cient\\'ifica C.U., 04510 Coyoacan, Ciudad de Mexico, Mexico'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Theoretical Physics and Center for Extreme Matter and Emergent Phenomena, Utrecht University, Leuvenlaan 4, 3584 CE Utrecht, The Netherlands'.\n",
      "WARNING:root:No OpenAlex information found for 'Asia Pacific Center for Theoretical Physics, Pohang, 37673, Korea'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Pohang University of Science and Technology, Pohang, 37673, Korea'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Theoretical Physics, University of Amsterdam, 1090 GL Amsterdam, The Netherlands'.\n",
      "WARNING:root:No OpenAlex information found for 'Dutch Institute for Emergent Phenomena (DIEP), University of Amsterdam, 1090 GL Amsterdam, The Netherlands'.\n",
      "WARNING:root:No OpenAlex information found for 'University of Aveiro, DETI'.\n",
      "WARNING:root:No OpenAlex information found for 'Instituto de Telecomunicações \\\\ University of Aveiro, DETI'.\n",
      "WARNING:root:No OpenAlex information found for '\\institution{University of Aveiro, DETI} \\country{Portugal}'.\n",
      "WARNING:root:No OpenAlex information found for '\\institution{Instituto de Telecomunicações \\\\ University of Aveiro, DETI} \\country{Portugal}'.\n",
      "WARNING:root:No OpenAlex information found for 'Ariel University, Ariel 40700, Israel\\\\ (E-mail: {\\tt asya@ariel.ac.il})'.\n",
      "WARNING:root:No OpenAlex information found for 'yyy'.\n",
      "WARNING:root:No OpenAlex information found for 'Sandia National Laboratories, Albuquerque, NM 87185, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Computing Research, Sandia National Laboratories, Albuquerque, NM 87185, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Max Planck Institute for Astrophysics, Garching, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Astronomy, Yale University, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Stellar Astrophysics Centre, Aarhus, Denmark'.\n",
      "WARNING:root:No OpenAlex information found for 'Anton Pannekoek Institute for Astronomy and GRAPPA, University of Amsterdam, The Netherlands'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics \\& Astronomy ``Augusto Righi,'' University of Bologna, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Physics and Astronomy, University of Birmingham, UK'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Astronomy \\& Space Science, University of the Chinese Academy of Sciences, Beijing, China'.\n",
      "WARNING:root:No OpenAlex information found for 'National Astronomical Observatories, Chinese Academy of Sciences, Beijing, China'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Physics and Astronomy, Tel-Aviv University, Tel-Aviv 69978, Israel'.\n",
      "WARNING:root:No OpenAlex information found for 'Racah Institute of Physics, Hebrew University of Jerusalem, Jerusalem 91904, Israel'.\n",
      "WARNING:root:No OpenAlex information found for 'Laboratoire d'Annecy-le-Vieux de Physique Th\\'eorique, CNRS -- USMB, BP 110 Annecy-le-Vieux, F-74941 Annecy, France'.\n",
      "WARNING:root:No OpenAlex information found for 'Neutron Scattering Division, Oak Ridge National Laboratory, Oak Ridge, Tennessee, 37831, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Quantum Science Center, Oak Ridge National Laboratory, Oak Ridge, Tennessee, 37831, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Center For Nanophase Materials Sciences, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, The University of Tennessee, Knoxville, TN 37996, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Advanced Materials \\& Manufacturing, The University of Tennessee, Knoxville, TN 37920, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'National Renewable Energy Laboratory, Golden, CO 80401, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Materials Science and Technology Division, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'ISIS Neutron and Muon Source, STFC Rutherford Appleton Laboratory, Didcot OX11 0QX, United Kingdom'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, Purdue University, West Lafayette IN - 47907, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Shull Wollan Center - A Joint Institute for Neutron Sciences, Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, Johns Hopkins University, Baltimore, Maryland 21218, USA\\\\'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Biophysics, Johns Hopkins University, Baltimore, Maryland 21218, USA\\\\'.\n",
      "WARNING:root:No OpenAlex information found for '\\href{http://www.ncbj.gov.pl}{National Centre for Nuclear Research}'.\n",
      "WARNING:root:No OpenAlex information found for 'Tata Institute of Fundamental Research, 1 Homi Bhabha Road, Mumbai 400005, India'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute of Plasma Research, Gandhinagar, Bhat, Gandhinagar 382428 India'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Indian Institute of Technology Delhi, Hauz Khas, New Delhi 110016, India'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, College of Charleston, 66 George Street, Charleston, SC 29424, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Lawrence Livermore National Laboratory, P.O. Box 808, Livermore, CA 94550, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Harish-Chandra Research Institute, HBNI Chhatnag Road, Jhusi, Allahabad 211019, India'.\n",
      "WARNING:root:No OpenAlex information found for 'Kavli Institute for Theoretical Physics, University of California Santa Barbara, Santa Barbara, CA 93106, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute of Multi-messenger Astrophysics and Cosmology \\& Physics Department, Missouri University of Science and Technology, Rolla, MO 65409, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, University of Mississippi, University, Mississippi 38677, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Dipartimento di Fisica, Universit\\`{a} di Torino \\& INFN Sezione di Torino, via P. Giuria 1, 10125 Torino, Italy'.\n",
      "ERROR:root:OpenAlex API query failed with status code: 500 for institution: Theoretical Astrophysics Department, Eberhard-Karls University of T\\\"{u}bingen, T\\\"{u}bingen 72076, Germany\n",
      "WARNING:root:No OpenAlex information found for 'Departament de Matem\\`{a}tiques, Universitat Aut\\`{o}noma de Barcelona, 08193 Bellaterra, Spain'.\n",
      "WARNING:root:No OpenAlex information found for 'Departament d'Astronomia i Astrof\\'{i}sica, Universitat de Val\\`{e}ncia, Dr. Moliner 50, 46100, Burjassot (Val\\`{e}ncia), Spain'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Joint Quantum Institute, University of Maryland, College Park, Maryland 20742, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Brandeis University, Waltham, Massachusetts 02453, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'P.N. Lebedev Institute of Physics, Lenin Avenue 53, 119991 Moscow, Russia'.\n",
      "WARNING:root:No OpenAlex information found for 'Skobeltsyn Institute of Nuclear Physics, Lomonosov Moscow State University, 119991 Moscow, Russia'.\n",
      "WARNING:root:No OpenAlex information found for 'Joint Institute for Nuclear Research, Dubna 141980, Moscow region, Russia'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Gravitation \\& the Cosmos, The Pennsylvania State University, University Park PA 16802, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, University of California, Berkeley, CA 94720, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, The Pennsylvania State University, University Park PA 16802, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Astronomy \\& Astrophysics, The Pennsylvania State University, University Park PA 16802, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Theoretisch-Physikalisches Institut, Friedrich-Schiller-Universit{\\\"a}t Jena, 07743, Jena, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'Dipartimento di Fisica, Università di Trento, Via Sommarive 14, 38123 Trento, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'INFN-TIFPA, Trento Institute for Fundamental Physics and Applications, via Sommarive 14, I-38123 Trento, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'LERMA, Observatoire de Paris, PSL Research University, CNRS, Sorbonne Universit\\'{e}, F-75014 Paris, France \\and Laboratoire de Physique de l’ENS, ENS, Universit\\'{e} PSL, CNRS, Sorbonne Universit\\'{e}, Universit\\'{e}e Paris Cit\\'{e}, 75005 Paris, France'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, York University, Toronto, ON, Canada, M3J 1P3'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801-3080, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Nordita, Stockholm University and KTH Royal Institute of Technology Hannes Alfv\\'ens v\\\"ag 12, SE-106 91 Stockholm, Sweden'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Physics of Microstructures, Russian Academy of Sciences, 603950 Nizhny Novgorod, GSP-105, Russia'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, University of California San Diego,\\\\ \\textit{9500 Gilman Drive, La Jolla CA 92093-0319, USA}'.\n",
      "ERROR:root:OpenAlex API query failed with status code: 500 for institution: Institut f\\\"ur Kernphysik, Johannes Gutenberg-Universit\\\"{a}t,\\\\ J.J. Becher-Weg 45, 55128 Mainz, Germany\n",
      "WARNING:root:No OpenAlex information found for 'PRISMA$^+$ Cluster of Excellence, Johannes Gutenberg-Universit\\\"{a}t,\\\\ Mainz, Germany\\\\ email: gorshtey@uni-mainz.de'.\n",
      "WARNING:root:No OpenAlex information found for 'Facility for Rare Isotope Beams, Michigan State University, East Lansing, MI 48824, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, University of Washington, Seattle, WA 98195-1560, USA\\\\ email: seng@frib.msu.edu'.\n",
      "WARNING:root:No OpenAlex information found for '\\footnotemark[2] \\orgdiv{Equipe IDEFIX}, \\orgname{INRIA Saclay} , \\orgaddress{UMA, ENSTA Paris, 828, Boulevard des Marechaux}, \\postcode{91762} \\city{Palaiseau}, \\country{France}'.\n",
      "WARNING:root:No OpenAlex information found for '\\footnote[2]{(Current address)} \\orgdiv{Department of Mathematics}, \\orgname{Hampton University}, \\\\ \\orgaddress{\\street{609 Norma B Harvey Road}, \\city{Hampton}, \\postcode{23669}, \\state{Virginia}, \\country{U.S.}}'.\n",
      "WARNING:root:No OpenAlex information found for 'Kavli Institute for Theoretical Physics, University of California, Santa Barbara, CA 93106, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, University of Massachusetts, Amherst, MA 01003, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ 08544, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Astronomy, and Quantum Matter Institute, University of British Columbia, Vancouver, BC, Canada V6T 1Z1'.\n",
      "WARNING:root:No OpenAlex information found for 'Physik-Institut, Universit\\\"at Z\\\"urich, CH-8057 Z\\\"urich, Switzerland'.\n",
      "WARNING:root:No OpenAlex information found for 'PRISMA$^+$ Cluster of Excellence {\\em \\&} MITP, Johannes Gutenberg University, Mainz, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'Physics Department, King’s College London, Strand, London, WC2R 2LS, United Kingdom'.\n",
      "WARNING:root:No OpenAlex information found for 'National Centre for Radio Astrophysics, Tata Institute of Fundamental Research, Pune, India.'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Interdisciplinary Exploration \\& Research in Astrophysics (CIERA), Physics \\& Astronomy, Northwestern University, Evanston, IL 60202, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Courant Institute of Mathematical Sciences, New York University, New York, New York 10011, United States'.\n",
      "WARNING:root:No OpenAlex information found for 'Black Hole Initiative at Harvard University, 20 Garden St., Cambridge, MA 02138, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Astrophysics, Harvard \\& Smithsonian, 60 Garden St., Cambridge, MA 02138, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Relativistic Astrophysics, Georgia Institute of Technology, Howey Physics Bldg, 837 State St NW, Atlanta, GA 30332, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Theory and Computation, Harvard University, 60 Garden Street, Cambridge, MA 02138, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Computational Astrophysics, Flatiron Institute, New York, NY 10010, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics and Columbia Astrophysics Laboratory, Columbia University, Pupin Hall, New York, NY 10027, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Mathematics, University of California, Berkeley, CA 94720, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Computational Sciences, Korea Institute for Advanced Study, 02455, Seoul, South Korea'.\n",
      "WARNING:root:No OpenAlex information found for 'Applied Mathematics and Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Challenge Institute for Quantum Computation, University of California, Berkeley, CA 94720, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Mathematics, Pennsylvania State University, University Park, PA 16802, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Institute for Advanced Studies in Basic Sciences (IASBS), Zanjan 45137‑66731, Iran'.\n",
      "WARNING:root:No OpenAlex information found for 'Optics Research Center, Institute for Advanced Studies in Basic Sciences (IASBS), Zanjan 45137-66731, Iran'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Florida Atlantic University, Boca Raton, Florida 33431, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Akdeniz University, Faculty of Sciences, Department of Space Sciences and Technologies, 07058, Antalya, Turkey'.\n",
      "WARNING:root:No OpenAlex information found for 'Istanbul University, Faculty of Science, Department of Astronomy and Space Sciences, 34119, Beyaz\\i t, Istanbul, Turkey'.\n",
      "WARNING:root:No OpenAlex information found for 'Nielsen, 675 6th Ave, New York, NY 10011, USA'.\n",
      "WARNING:root:No OpenAlex information found for 'Dept.\\ of Physical Science \\& Engineering, Harper College, 1200 W Algonquin Rd, IL 60067, USA'.\n",
      "WARNING:root:No OpenAlex information found for '$^1$ Department of Physics, Ben-Gurion University of the Negev, \\\\ David Ben Gurion Boulevard 1, Beer Sheva 84105, Israel \\\\'.\n",
      "WARNING:root:No OpenAlex information found for '$^1$Department of Physics, University of California Santa Barbara, Santa Barbara, California 93106, USA\\\\ $^2$ Institute of Physics, University of Amsterdam, Science Park 904, PO Box 94485, 1090 GL Amsterdam, The Netherlands\\\\ $^3$ Delta Institute for Theoretical Physics, Science Park 904, PO Box 94485, 1090 GL Amsterdam, The Netherlands'.\n",
      "WARNING:root:No OpenAlex information found for 'organization={Department of Computer Science, University of Saskatchewan}, city={Saskatoon}, state={Saskatchewan}, country={Canada}'.\n",
      "WARNING:root:No OpenAlex information found for '\\orgdiv{Clarkson Center for Complex Systems Science}, \\orgname{Clarkson University}, \\orgaddress{\\street{8 Clarkson Ave.}, \\city{Potsdam}, \\postcode{13699}, \\state{New York}, \\country{U.S.A}}'.\n",
      "WARNING:root:No OpenAlex information found for '\\orgdiv{Instituto de Ci\\^encias Matem\\'aticas e Computa\\c{c}\\~ao}, \\orgname{Universidade de S\\~ao Paulo}, \\orgaddress{\\street{Av. Trab. São Carlense, 400 }, \\city{S\\~ao Carlos}, \\postcode{ 13566-590}, \\state{S\\~ao Paulo}, \\country{Brazil}}'.\n",
      "WARNING:root:No OpenAlex information found for '\\orgdiv{Department of Radiology}, \\orgname{Wake Forest University School of Medicine}, \\orgaddress{\\street{475 Vine Street}, \\city{Winston-Salem}, \\postcode{27101}, \\state{North Carolina}, \\country{U.S.A}}'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Theoretical Physics, University of Innsbruck, Innsbruck 6020, Austria'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Quantum Optics and Quantum Information, Austrian Academy of Sciences, Innsbruck 6020, Austria'.\n",
      "WARNING:root:No OpenAlex information found for 'Institut f\\\"ur Theoretische Physik, Universit\\\"at T\\\"ubingen, Auf der Morgenstelle 14, 72076 T\\\"ubingen, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'Institute for Applied Physics, University of Bonn, Wegelerstraße 8, 53115 Bonn, Germany'.\n",
      "WARNING:root:No OpenAlex information found for 'School of Physics and Astronomy and Centre for the Mathematics and Theoretical Physics of Quantum Non-Equilibrium Systems, The University of Nottingham, Nottingham, NG7 2RD, United Kingdom'.\n",
      "WARNING:root:No OpenAlex information found for 'Department of Physics, Istanbul Technical University, Maslak 34469 Istanbul, T\\\"urkiye'.\n",
      "WARNING:root:No OpenAlex information found for 'Center for Joint Quantum Studies and Department of Physics,\\\\ School of Science, Tianjin University, Tianjin 300350, China \\\\'.\n",
      "WARNING:root:No OpenAlex information found for 'Dipartimento di Fisica, Universit\\`a di Torino, Via P. Giuria 1, 10125 Torino, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'Istituto Nazionale di Fisica Nucleare, Sezione di Torino, Via P. Giuria 1, 10125 Torino, Italy'.\n",
      "WARNING:root:No OpenAlex information found for 'LAPTh, CNRS, F-74000 Annecy, France'.\n",
      "WARNING:root:No OpenAlex information found for '$^1$Scottish Universities Physics Alliance, School of Physics and Astronomy, University of St. Andrews, KY16 9SS, United Kingdom \\\\ $^2$Department of Physics, Simon Fraser University, Burnaby, British Columbia V5A 1S6, Canada \\\\ $^3$Centre for Molecular and Materials Science, TRIUMF, Vancouver, British Columbia V6T 2A3, Canada \\\\ $^4$Department of Physics and Astronomy, University of California, Davis, California 95616, USA'.\n",
      "WARNING:root:No OpenAlex information found for '$^*$Physics Department, Arizona State University, Tempe, Arizona 85287, USA.\\\\ $^\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.'.\n",
      "WARNING:root:No OpenAlex information found for '\\small Department of Physics, Swansea University,\\protect\\\\ \\small Swansea SA2 8PP, United Kingdom \\vspace{0.2cm}'.\n",
      "WARNING:root:No OpenAlex information found for '\\small Yau Mathematical Sciences Center,\\protect\\\\ \\small Tsinghua University, Beijing, 100084, China \\vspace{0.2cm}'.\n",
      "WARNING:root:No OpenAlex information found for '\\small Nordita, KTH Royal Institute of Technology and Stockholm University,\\protect\\\\ \\small Hannes Alfv\\'ens V\\\"ag 12, 106 91 Stockholm, Sweden \\vspace{0.2cm}'.\n",
      "WARNING:root:No OpenAlex information found for '\\small Niels Bohr Institute, Copenhagen University,\\protect\\\\ \\small Blegdamsvej 17, 2100 Copenhagen, Denmark \\vspace{0.2cm}'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON export completed. Results saved to institution_output_with_ror.json\n"
     ]
    }
   ],
   "source": [
    "# Process each paper\n",
    "for paper, macros in extracted_data.items():\n",
    "    unique_openalex_ids = {}  # Dictionary to store unique OpenAlex results\n",
    "\n",
    "    if set(macros.keys()) in [{\"full_content\"}, {\"address\"}]:\n",
    "        # Extract from full content or address\n",
    "        text = \"\\n\".join(macros.get(\"full_content\", [])) + \"\\n\" + \"\\n\".join(macros.get(\"address\", []))\n",
    "        extracted_openalex_ids = extractor.extract_affiliations(text)\n",
    "\n",
    "        if extracted_openalex_ids:\n",
    "            unique_openalex_ids.update({openalex_id: {\"OpenAlex_ID\": openalex_id} for openalex_id in extracted_openalex_ids})\n",
    "\n",
    "    else:\n",
    "        # Process institution names in macros (excluding full_content & address)\n",
    "        for content_list in macros.values():\n",
    "            for institution_name in content_list:\n",
    "                openalex_info = query_openalex(institution_name)\n",
    "                if openalex_info:\n",
    "                    unique_openalex_ids[openalex_info[\"OpenAlex_ID\"]] = openalex_info  # Store unique OpenAlex results\n",
    "\n",
    "    # Save results if there are OpenAlex matches\n",
    "    if unique_openalex_ids:\n",
    "        results[paper] = list(unique_openalex_ids.values())  # Convert to list for JSON storage\n",
    "\n",
    "# Save results to JSON file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ JSON export completed. Results saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Column(s) ['Extracted ROR ID'] do not exist\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(item\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m extracted_ror_id \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m grouped \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mArXiv Id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mROR ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Create a set of ROR IDs\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExtracted ROR ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_extracted_ror_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Preprocess and union Extracted ROR IDs\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Step 3: Calculate accuracy and wrong extraction rate\u001b[39;00m\n\u001b[1;32m     22\u001b[0m correct_extractions \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/groupby/generic.py:894\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m func \u001b[38;5;241m=\u001b[39m maybe_mangle_lambdas(func)\n\u001b[1;32m    893\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args, kwargs)\n\u001b[0;32m--> 894\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:169\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(arg):\n\u001b[0;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(arg):\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:478\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     selected_obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selected_obj\n\u001b[1;32m    476\u001b[0m     selection \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_selection\n\u001b[0;32m--> 478\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_dictlike_arg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m selected_obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# key only used for output\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     colg \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_gotitem(selection, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/apply.py:601\u001b[0m, in \u001b[0;36mApply.normalize_dictlike_arg\u001b[0;34m(self, how, obj, func)\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m         cols_sorted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(safe_sort(\u001b[38;5;28mlist\u001b[39m(cols)))\n\u001b[0;32m--> 601\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcols_sorted\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m do not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m aggregator_types \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# if we have a dict of any non-scalars\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# eg. {'A' : ['mean']}, normalize all to\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;66;03m# be list-likes\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;66;03m# Cannot use func.values() because arg may be a Series\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Column(s) ['Extracted ROR ID'] do not exist\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Calculate accuracy and wrong extraction rate\n",
    "correct_extractions = 0\n",
    "wrong_extractions = 0\n",
    "total_ror_ids = 0\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids:  # Only consider rows with non-empty ROR ID\n",
    "        total_ror_ids += len(ror_ids)\n",
    "        correct_extractions += len(ror_ids.intersection(extracted_ror_ids))\n",
    "        wrong_extractions += len(extracted_ror_ids - ror_ids)  # IDs in extraction but not in ground truth\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "# Calculate wrong extraction rate\n",
    "wrong_extraction_rate = wrong_extractions / total_ror_ids if total_ror_ids > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Wrong Extraction Rate: {wrong_extraction_rate:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('data/2201.00_scopus_931_with_ror.csv')\n",
    "\n",
    "# Function to preprocess Extracted ROR ID\n",
    "def preprocess_extracted_ror_id(extracted_ror_id):\n",
    "    if pd.isna(extracted_ror_id):  # Skip NaN values\n",
    "        return set()\n",
    "    # Remove curly braces and split by comma\n",
    "    extracted_ror_id = extracted_ror_id.strip(\"{}\").split(\",\")\n",
    "    # Remove any leading/trailing whitespace and filter out empty strings\n",
    "    return set(item.strip().strip(\"''\") for item in extracted_ror_id if item.strip())\n",
    "\n",
    "# Step 2: Group by ArXiv ID and merge ROR ID and Extracted ROR ID into sets\n",
    "grouped = df.groupby('ArXiv Id').agg({\n",
    "    'ROR ID': lambda x: set(x.dropna()),  # Create a set of ROR IDs\n",
    "    'Extracted ROR ID': lambda x: set().union(*x.apply(preprocess_extracted_ror_id))  # Preprocess and union Extracted ROR IDs\n",
    "}).reset_index()\n",
    "\n",
    "# Step 3: Collect cases where extracted ROR IDs do not perfectly match ground truth ROR IDs\n",
    "mismatched_cases = []\n",
    "\n",
    "for index, row in grouped.iterrows():\n",
    "    ror_ids = row['ROR ID']\n",
    "    extracted_ror_ids = row['Extracted ROR ID']\n",
    "    if ror_ids != extracted_ror_ids:  # Check for imperfect match\n",
    "        mismatched_cases.append({\n",
    "            'ArXiv Id': row['ArXiv Id'],\n",
    "            'Ground Truth ROR IDs': ror_ids,\n",
    "            'Extracted ROR IDs': extracted_ror_ids,\n",
    "            'Missing ROR IDs': ror_ids - extracted_ror_ids,  # IDs in ground truth but not in extraction\n",
    "            'Extra ROR IDs': extracted_ror_ids - ror_ids  # IDs in extraction but not in ground truth\n",
    "        })\n",
    "\n",
    "# Convert the list of mismatched cases to a DataFrame\n",
    "mismatched_df = pd.DataFrame(mismatched_cases)\n",
    "\n",
    "# Save the mismatched cases to a CSV file\n",
    "mismatched_df.to_csv('mismatched_cases.csv', index=False)\n",
    "\n",
    "print(f\"Total mismatched cases: {len(mismatched_df)}\")\n",
    "print(\"Mismatched cases saved to 'mismatched_cases.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded persistent ROR cache.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configure ROR API information\n",
    "ROR_SEARCH_URL = 'https://api.ror.org/organizations'\n",
    "\n",
    "# Initialize ROR cache\n",
    "ror_cache = {}\n",
    "\n",
    "# Load persistent cache\n",
    "cache_file = 'ror_cache.json'\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        ror_cache = json.load(f)\n",
    "    logging.info(\"Loaded persistent ROR cache.\")\n",
    "else:\n",
    "    logging.info(\"No persistent ROR cache found. Using empty cache.\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
    "def query_ror(institution_name):\n",
    "    \"\"\"\n",
    "    Query ROR information, returning details from the first result in the response.\n",
    "    \"\"\"\n",
    "    if institution_name in ror_cache:\n",
    "        return ror_cache[institution_name]\n",
    "    \n",
    "    params = {'query': institution_name}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(ROR_SEARCH_URL, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('items'):\n",
    "                first_result = data['items'][0]  # Get the first search result\n",
    "                \n",
    "                ror_cache[institution_name] = {\n",
    "                    'ROR_ID': first_result.get('id', 'N/A'),\n",
    "                    'Name': first_result.get('name', 'N/A'),\n",
    "                    'Country': first_result.get('country', {}).get('country_name', 'N/A'),\n",
    "                    'Type': ', '.join(first_result.get('types', []))\n",
    "                }\n",
    "                logging.info(f\"First ROR result found for '{institution_name}': {first_result.get('id', 'N/A')}\")\n",
    "                return ror_cache[institution_name]\n",
    "\n",
    "            else:\n",
    "                logging.warning(f\"No ROR information found for '{institution_name}'.\")\n",
    "                ror_cache[institution_name] = None\n",
    "                return None\n",
    "        else:\n",
    "            logging.error(f\"ROR API query failed with status code: {response.status_code} for institution: {institution_name}\")\n",
    "            ror_cache[institution_name] = None\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ROR API query exception for '{institution_name}', Error: {e}\")\n",
    "        ror_cache[institution_name] = None\n",
    "        return None\n",
    "\n",
    "\n",
    "# Save cache to a file after all queries\n",
    "def save_cache():\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ror_cache, f)\n",
    "    logging.info(\"Saved ROR cache to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:First ROR result found for '\footnote(Current address) \\orgdivDepartment of Mathematics, \\orgnameHampton University, \\ \\orgaddress\\street609 Norma B Harvey Road, \\cityHampton, \\postcode23669, \\stateVirginia, \\countryU.S.': https://ror.org/025t37b39\n",
      "WARNING:root:No OpenAlex information found for '\footnote(Current address) \\orgdivDepartment of Mathematics, \\orgnameHampton University, \\ \\orgaddress\\street609 Norma B Harvey Road, \\cityHampton, \\postcode23669, \\stateVirginia, \\countryU.S.'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From ROR: {\n",
      "    \"ROR_ID\": \"https://ror.org/025t37b39\",\n",
      "    \"Name\": \"B & B\",\n",
      "    \"Country\": \"Slovenia\",\n",
      "    \"Type\": \"Education\"\n",
      "}\n",
      "From OpenAlex: null\n"
     ]
    }
   ],
   "source": [
    "institution_name = '\\footnote(Current address) \\orgdivDepartment of Mathematics, \\orgnameHampton University, \\\\ \\orgaddress\\street609 Norma B Harvey Road, \\cityHampton, \\postcode23669, \\stateVirginia, \\countryU.S.'\n",
    "result1 = query_ror(institution_name)\n",
    "result2 = query_openalex(institution_name)\n",
    "print('From ROR:', json.dumps(result1, indent=4, ensure_ascii=False))\n",
    "print('From OpenAlex:', json.dumps(result2, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: University of Minnesota, Minneapolis, MN 55455, USA\n",
      "Cleaned: University of Minnesota, Minneapolis, MN 55455\n",
      "\n",
      "Original: Stockholm University, Stockholm 106 91, Sweden\n",
      "Cleaned: Stockholm University, Stockholm 106 91\n",
      "\n",
      "Original: Technical University of Munich, Germany; Institute for Advanced Study, D-85748 Garching, Germany\n",
      "Cleaned: Technical University of Munich, Germany; Institute for Advanced Study, D-\n",
      "\n",
      "Original: Institut für Theoretische Physik, Universität Heidelberg, Philosophenweg 16, 69120, Heidelberg, Germany\n",
      "Cleaned: Institut für Theoretische Physik, Universität Heidelberg, Philosophenweg 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Load a list of countries and cities\n",
    "geolocator = Nominatim(user_agent=\"geo_cleaner\")\n",
    "\n",
    "# Example known city, country patterns (you can expand this)\n",
    "COUNTRY_CITY_PATTERNS = [\n",
    "    r\",?\\s*(USA|United States|UK|Germany|France|Italy|Sweden|Canada|China|India|Japan)$\",\n",
    "    r\",?\\s*[A-Za-z\\s]+,\\s*(USA|UK|Germany|France|Italy|Sweden|Canada|China|India|Japan)$\",\n",
    "    r\",?\\s*\\d{5,},?\\s*[A-Za-z\\s]+$\",  # Matches postal codes and city names\n",
    "    r\",?\\s*(Department of|Faculty of|School of|Institute for|University of)$\",  # Matches trailing dept names\n",
    "]\n",
    "\n",
    "def clean_institution_name(name):\n",
    "    \"\"\"Removes address-related parts from an institution name\"\"\"\n",
    "    cleaned_name = name\n",
    "\n",
    "    # Try to match and remove known patterns\n",
    "    for pattern in COUNTRY_CITY_PATTERNS:\n",
    "        cleaned_name = re.sub(pattern, \"\", cleaned_name, flags=re.IGNORECASE).strip()\n",
    "\n",
    "    return cleaned_name\n",
    "\n",
    "# Example test cases\n",
    "institution_names = [\n",
    "    \"University of Minnesota, Minneapolis, MN 55455, USA\",\n",
    "    \"Stockholm University, Stockholm 106 91, Sweden\",\n",
    "    \"Technical University of Munich, Germany; Institute for Advanced Study, D-85748 Garching, Germany\",\n",
    "    \"Institut für Theoretische Physik, Universität Heidelberg, Philosophenweg 16, 69120, Heidelberg, Germany\"\n",
    "]\n",
    "\n",
    "for name in institution_names:\n",
    "    print(f\"Original: {name}\")\n",
    "    print(f\"Cleaned: {clean_institution_name(name)}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: University of Minnesota, Minneapolis, MN 55455, USA\n",
      "Cleaned: University of Minnesota 55455\n",
      "\n",
      "Original: Stockholm University, Stockholm 106 91, Sweden\n",
      "Cleaned: Stockholm University 106 91\n",
      "\n",
      "Original: Technical University of Munich, Germany; Institute for Advanced Study, D-85748 Garching, Germany\n",
      "Cleaned: Technical University of Munich Institute for Advanced Study D-85748 Garching\n",
      "\n",
      "Original: Institut für Theoretische Physik, Universität Heidelberg, Philosophenweg 16, 69120, Heidelberg, Germany\n",
      "Cleaned: Physik Universität Heidelberg Philosophenweg 16, 69120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_address_spacy(text):\n",
    "    \"\"\"Uses spaCy NER to remove location entities from an institution name\"\"\"\n",
    "    doc = nlp(text)\n",
    "    cleaned_parts = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"}:  # GPE = Geo-Political Entity (City, Country, etc.)\n",
    "            continue  # Skip locations\n",
    "        cleaned_parts.append(ent.text)\n",
    "\n",
    "    return \" \".join(cleaned_parts)\n",
    "\n",
    "# Example usage\n",
    "test_institutions = [\n",
    "    \"University of Minnesota, Minneapolis, MN 55455, USA\",\n",
    "    \"Stockholm University, Stockholm 106 91, Sweden\",\n",
    "    \"Technical University of Munich, Germany; Institute for Advanced Study, D-85748 Garching, Germany\",\n",
    "    \"Institut für Theoretische Physik, Universität Heidelberg, Philosophenweg 16, 69120, Heidelberg, Germany\"\n",
    "]\n",
    "\n",
    "for name in test_institutions:\n",
    "    print(f\"Original: {name}\")\n",
    "    print(f\"Cleaned: {remove_address_spacy(name)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Institut f\\\"{u}r Theoretische Physik, Universit\\\"{a}t Heidelberg\n",
      "Converted: Institut für Theoretische Physik, Universität Heidelberg\n",
      "\n",
      "Original: Dipartimento di Fisica ``E.R. Caianiello'', Università degli Studi di Salerno\n",
      "Converted: Dipartimento di Fisica “E.R. Caianiello”, Università degli Studi di Salerno\n",
      "\n",
      "Original: Technische Universit\\\"{a}t M\\\"{u}nchen\n",
      "Converted: Technische Universität München\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "converter = LatexNodes2Text()\n",
    "\n",
    "def latex_to_unicode(text):\n",
    "    \"\"\"Converts LaTeX special characters to Unicode.\"\"\"\n",
    "    return converter.latex_to_text(text)\n",
    "\n",
    "# Example conversion\n",
    "test_strings = [\n",
    "    r\"Institut f\\\"{u}r Theoretische Physik, Universit\\\"{a}t Heidelberg\",\n",
    "    r\"Dipartimento di Fisica ``E.R. Caianiello'', Università degli Studi di Salerno\",\n",
    "    r\"Technische Universit\\\"{a}t M\\\"{u}nchen\",\n",
    "]\n",
    "\n",
    "for s in test_strings:\n",
    "    print(f\"Original: {s}\")\n",
    "    print(f\"Converted: {latex_to_unicode(s)}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
