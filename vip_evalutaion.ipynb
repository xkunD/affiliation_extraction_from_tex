{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no ror id for  Austrian Science Fund\n",
      "no ror id for  CSIC - EstaciÃ³n Experimental La Mayora (EELM)\n",
      "no ror id for  CSIC-UZA - Instituto de Ciencia de Materiales de Aragon (ICMA)\n",
      "no ror id for  Campus de Excelencia Internacional UAM+CSIC\n",
      "no ror id for  Centre pour la Communication Scientifique Directe\n",
      "no ror id for  FinElib Consortium\n",
      "no ror id for  NII Japan Consortium\n",
      "no ror id for  Niels Bohr Institute\n",
      "no ror id for  UCLouvain\n",
      "no ror id for  arXiv-DH and Helmholtz Association of German Research Centres (HGF) (DHHGF)\n",
      "Precision: 0.8002\n",
      "Recall:    0.7819\n",
      "F1 Score:  0.7910\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "vip_csv = \"dashboard_institutions2023_2025-01-21.csv\"\n",
    "unfiltered_result_from_llm = \"final_affiliations_2000_parallel.json\"\n",
    "filtered_cascaded_result = \"result_combined.json\"\n",
    "ground_truth = \"data/2311_with_ror.csv\"\n",
    "\n",
    "vip_rors = set()\n",
    "with open(vip_csv, encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ror_id = row['ror_id'].strip()\n",
    "        if ror_id:\n",
    "            vip_rors.add(ror_id)\n",
    "        else:\n",
    "            print(\"no ror id for \", row['name'])\n",
    "\n",
    "# only for getting all examined paper ids\n",
    "with open(unfiltered_result_from_llm, encoding='utf-8') as f:\n",
    "    final_affiliations = json.load(f)\n",
    "eval_paper_ids = set(entry[\"File ID\"] for entry in final_affiliations)\n",
    "\n",
    "paper_to_true_rors = defaultdict(set)\n",
    "with open(ground_truth, encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        paper_id = row['paper_id'].strip()\n",
    "        ror_id = row['ROR ID'].strip()\n",
    "        if paper_id in eval_paper_ids and ror_id in vip_rors:\n",
    "            paper_to_true_rors[paper_id].add(ror_id)\n",
    "\n",
    "# this result has already filtered to only include VIP RORs by Yi\n",
    "with open(filtered_cascaded_result, encoding='utf-8') as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "\n",
    "confusion_matrix = {ror: {\"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0} for ror in vip_rors}\n",
    "\n",
    "for paper_id in eval_paper_ids:\n",
    "    true_rors = paper_to_true_rors.get(paper_id, set())\n",
    "    pred_rors = set(predictions.get(paper_id, []))  # may be missing\n",
    "\n",
    "    for vip_ror in vip_rors:\n",
    "        in_truth = vip_ror in true_rors\n",
    "        in_pred = vip_ror in pred_rors\n",
    "\n",
    "        if in_truth and in_pred:\n",
    "            confusion_matrix[vip_ror][\"TP\"] += 1\n",
    "        elif not in_truth and in_pred:\n",
    "            confusion_matrix[vip_ror][\"FP\"] += 1\n",
    "        elif in_truth and not in_pred:\n",
    "            confusion_matrix[vip_ror][\"FN\"] += 1\n",
    "        elif not in_truth and not in_pred:\n",
    "            confusion_matrix[vip_ror][\"TN\"] += 1\n",
    "\n",
    "# Print per-institution results\n",
    "def safe_divide(num, denom):\n",
    "    return num / denom if denom != 0 else 0.0\n",
    "\n",
    "# Add per-institution metrics\n",
    "metrics_per_vip = {}\n",
    "for ror, stats in confusion_matrix.items():\n",
    "    tp = stats[\"TP\"]\n",
    "    fp = stats[\"FP\"]\n",
    "    fn = stats[\"FN\"]\n",
    "\n",
    "    precision = safe_divide(tp, tp + fp)\n",
    "    recall = safe_divide(tp, tp + fn)\n",
    "    f1 = safe_divide(2 * precision * recall, precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    metrics_per_vip[ror] = {\n",
    "        **stats,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# total TP/FP/FN over all VIPs\n",
    "total_tp = sum(m[\"TP\"] for m in metrics_per_vip.values())\n",
    "total_fp = sum(m[\"FP\"] for m in metrics_per_vip.values())\n",
    "total_fn = sum(m[\"FN\"] for m in metrics_per_vip.values())\n",
    "\n",
    "micro_precision = safe_divide(total_tp, total_tp + total_fp)\n",
    "micro_recall = safe_divide(total_tp, total_tp + total_fn)\n",
    "micro_f1 = safe_divide(2 * micro_precision * micro_recall, micro_precision + micro_recall)\n",
    "\n",
    "\n",
    "print(f\"Precision: {micro_precision:.4f}\")\n",
    "print(f\"Recall:    {micro_recall:.4f}\")\n",
    "print(f\"F1 Score:  {micro_f1:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Step 1: Load ROR ID to InstitutionName mapping\n",
    "ror_to_name = {}\n",
    "with open(\"dashboard_institutions_2023_2024-06-28.csv\", encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        ror_id = row[\"rorId\"].strip()\n",
    "        if ror_id:\n",
    "            ror_to_name[ror_id] = row[\"InstitutionName\"].strip()\n",
    "\n",
    "# Step 2: Convert confusion_matrix to DataFrame\n",
    "df = pd.DataFrame.from_dict(confusion_matrix, orient=\"index\")\n",
    "df.index.name = \"ROR ID\"\n",
    "\n",
    "# Step 3: Add InstitutionName as a column\n",
    "df[\"InstitutionName\"] = df.index.map(ror_to_name)\n",
    "\n",
    "# Optional: move InstitutionName to first column\n",
    "df = df.reset_index()\n",
    "df = df[[\"ROR ID\", \"TP\", \"FP\", \"FN\", \"TN\", \"InstitutionName\"]]\n",
    "\n",
    "# Step 4: Save to CSV\n",
    "df.to_csv(\"vip_confusion_matrix.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the VIP dashboard into a DataFrame\n",
    "dashboard_path = \"dashboard_institutions_2023_2024-06-28.csv\"\n",
    "dashboard_df = pd.read_csv(dashboard_path,dtype={\n",
    "        \"salsaId\": str,\n",
    "        \"orgId\": str,\n",
    "        \"sId\": str,\n",
    "        \"isConsortium\": str  # optional\n",
    "    })\n",
    "\n",
    "# Prepare evaluation results as a DataFrame\n",
    "eval_rows = []\n",
    "for ror_id in vip_rors:\n",
    "    row = confusion_matrix.get(ror_id, {\n",
    "        \"TP\": 0, \"FP\": 0, \"FN\": 0, \"TN\": 0\n",
    "    })\n",
    "    row_with_ror = {\"rorId\": ror_id, **row}\n",
    "    eval_rows.append(row_with_ror)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "\n",
    "# Merge with dashboard on rorId\n",
    "merged_df = dashboard_df.merge(eval_df, on=\"rorId\", how=\"left\")\n",
    "\n",
    "# Fill NaNs with 0 for institutions not matched\n",
    "for col in [\"TP\", \"FP\", \"FN\", \"TN\"]:\n",
    "    merged_df[col] = merged_df[col].fillna(0).astype(int)\n",
    "\n",
    "# Save to new CSV\n",
    "merged_df.to_csv(\"dashboard_institutions_with_raw_counts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Paper-Level VIP Prediction Evaluation ===\n",
      "Total TP: 1434, FP: 358, FN: 400\n",
      "Micro Precision: 0.8002\n",
      "Micro Recall:    0.7819\n",
      "Micro F1 Score:  0.7910\n",
      "Avg Paper Accuracy (Jaccard-style): 0.8289\n"
     ]
    }
   ],
   "source": [
    "total_tp, total_fp, total_fn = 0, 0, 0\n",
    "paper_accuracies = []\n",
    "\n",
    "for paper_id in eval_paper_ids:\n",
    "    true_rors = paper_to_true_rors.get(paper_id, set())\n",
    "    pred_rors = set(predictions.get(paper_id, []))\n",
    "    \n",
    "    # Only keep VIP RORs\n",
    "    true_vip = true_rors & vip_rors\n",
    "    pred_vip = pred_rors & vip_rors\n",
    "\n",
    "    tp = len(true_vip & pred_vip)\n",
    "    fp = len(pred_vip - true_vip)\n",
    "    fn = len(true_vip - pred_vip)\n",
    "\n",
    "    total_tp += tp\n",
    "    total_fp += fp\n",
    "    total_fn += fn\n",
    "\n",
    "    denom = len(true_vip | pred_vip)\n",
    "    acc = tp / denom if denom > 0 else 1.0  # If both sets empty, consider fully accurate\n",
    "    paper_accuracies.append(acc)\n",
    "\n",
    "# Micro metrics\n",
    "micro_precision = safe_divide(total_tp, total_tp + total_fp)\n",
    "micro_recall = safe_divide(total_tp, total_tp + total_fn)\n",
    "micro_f1 = safe_divide(2 * micro_precision * micro_recall, micro_precision + micro_recall)\n",
    "\n",
    "# Average paper-level accuracy\n",
    "avg_paper_acc = sum(paper_accuracies) / len(paper_accuracies)\n",
    "\n",
    "print(\"\\n=== Paper-Level VIP Prediction Evaluation ===\")\n",
    "print(f\"Total TP: {total_tp}, FP: {total_fp}, FN: {total_fn}\")\n",
    "print(f\"Micro Precision: {micro_precision:.4f}\")\n",
    "print(f\"Micro Recall:    {micro_recall:.4f}\")\n",
    "print(f\"Micro F1 Score:  {micro_f1:.4f}\")\n",
    "print(f\"Avg Paper Accuracy (Jaccard-style): {avg_paper_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
