{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "import zipfile\n",
    "import importlib\n",
    "import regex as re\n",
    "import pyperclip  \n",
    "import TexSoup as TS\n",
    "from TexSoup.tokens import MATH_ENV_NAMES\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_doc_class(fp, name_match=False):\n",
    "    '''Search for document class related lines in a file and return a code to represent the type'''\n",
    "    doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "    sub_doc_class = re.compile(r\"^\\s*\\\\document(?:style|class).*(?:\\{standalone\\}|\\{subfiles\\})\")\n",
    "\n",
    "    # Read the content as bytes\n",
    "    file_content = fp.read()\n",
    "    try:\n",
    "        # Try decoding with UTF-8\n",
    "        file_text = file_content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 encoding if UTF-8 fails\n",
    "        file_text = file_content.decode('latin-1')\n",
    "\n",
    "    for line in file_text.splitlines():\n",
    "        if doc_class_pat.search(line):\n",
    "            if name_match:\n",
    "                if sub_doc_class.search(line):\n",
    "                    return -99999\n",
    "                return 1  # Found document class line\n",
    "    return 0  # No document class line found\n",
    "\n",
    "def find_main_tex_source_in_tar(tar_file, encoding='utf-8'):\n",
    "    tex_names = set([\"paper\", \"main\", \"ms.\", \"article\"])\n",
    "    tex_files = [f for f in tar_file.getnames() if f.endswith('.tex')]\n",
    "\n",
    "    if len(tex_files) == 1:\n",
    "        return tex_files[0]\n",
    "\n",
    "    main_files = {}\n",
    "    for tf in tex_files:\n",
    "        depth = len(tf.split('/')) - 1\n",
    "        has_main_name = any(kw in tf for kw in tex_names)\n",
    "        fp = tar_file.extractfile(tf)\n",
    "        if fp:\n",
    "            main_files[tf] = find_doc_class(fp, name_match=has_main_name) - depth\n",
    "            fp.close()\n",
    "\n",
    "    return max(main_files, key=main_files.get) if main_files else None\n",
    "\n",
    "def pre_format(text):\n",
    "    source_text = text.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }').replace(')$', ') $')\n",
    "    return source_text\n",
    "\n",
    "def source_from_tar(tar_file, encoding='utf-8'):\n",
    "    tex_main = find_main_tex_source_in_tar(tar_file, encoding=encoding)\n",
    "    if tex_main:\n",
    "        fp = tar_file.extractfile(tex_main)\n",
    "        if fp is not None:\n",
    "            file_content = fp.read()  # Read as bytes to keep it in memory\n",
    "            try:\n",
    "                # Attempt to decode using UTF-8\n",
    "                source_text = pre_format(file_content.decode(encoding))\n",
    "            except UnicodeDecodeError:\n",
    "                # Fallback to latin-1 encoding if UTF-8 fails\n",
    "                source_text = pre_format(file_content.decode('latin-1'))\n",
    "            return source_text\n",
    "    return None\n",
    "\n",
    "def extract_before_abstract(source_text):\n",
    "    no_comments_text = re.sub(r'(?<!\\\\)%.*', '', source_text)\n",
    "    no_usepackage_text = re.sub(r'\\\\usepackage\\s*\\{[^}]+\\}', '', no_comments_text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', no_usepackage_text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\[[^\\]]*\\]\\{[^}]*\\}', '', no_usepackage_text)\n",
    "    text = re.sub(r'\\$[^$]*\\$', '', no_usepackage_text)\n",
    "    text = no_usepackage_text.replace('{', '').replace('}', '').replace('\\n', ' ')\n",
    "    text = ' '.join(no_usepackage_text.split())\n",
    "    abstract_match = re.search(r'\\\\begin\\s*\\{\\s*abstract\\s*\\}', text)\n",
    "\n",
    "    if abstract_match:\n",
    "        return text[:abstract_match.start()].strip()\n",
    "    \n",
    "    abstract_word_match = re.search(r'\\babstract\\b', text, re.IGNORECASE)\n",
    "    if abstract_word_match:\n",
    "        return text[:abstract_word_match.start()].strip()\n",
    "    return None\n",
    "\n",
    "zip_file_path = \"./2401.zip\"\n",
    "output_path = \"./firstoutput.txt\"\n",
    "\n",
    "latest_versions = {}\n",
    "# Track the latest version of each identifier\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    tar_files = [f for f in zip_file.namelist() if f.endswith('.tar.gz')]\n",
    "    for tar_name in tar_files:\n",
    "        base_name, version = tar_name.rsplit(\"v\", 1)\n",
    "        version_num = int(version.split('.')[0])  # Extract version number\n",
    "        if base_name not in latest_versions or version_num > latest_versions[base_name][1]:\n",
    "            latest_versions[base_name] = (tar_name, version_num)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    for base_name, (tar_name, version) in latest_versions.items():\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            with zip_file.open(tar_name) as tar_bytes:\n",
    "                tar_file = tarfile.open(fileobj=io.BytesIO(tar_bytes.read()), mode='r:gz')\n",
    "                source_text = source_from_tar(tar_file)\n",
    "                if source_text:\n",
    "                    content_before_abstract = extract_before_abstract(source_text)\n",
    "                    if content_before_abstract:\n",
    "                        output_file.write(f\"Content before abstract in {tar_name}:\\n{content_before_abstract}\\n\\n\")\n",
    "                    else:\n",
    "                        output_file.write(f\"No abstract found in {tar_name}, or no content before abstract.\\n\\n\")\n",
    "                tar_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input from a text file and filter out files without valid content before the abstract\n",
    "input_file_path = 'firstoutput.txt'  # Replace with your actual file path\n",
    "output_file_path = 'filtered_files_with_content_macro.txt'\n",
    "\n",
    "# Variables to keep track of statistics\n",
    "total_files_count_author = 0\n",
    "valid_files_count = 0\n",
    "valid_files_with_content = []\n",
    "\n",
    "# Reading and processing the input file\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "    \n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count_author += 1\n",
    "            lines = block.split('\\n', 1)  # Split to separate the file name from its content\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                \n",
    "                # Check if the content does not indicate \"No abstract found\"\n",
    "                if 'No abstract found' not in content and 'no content before abstract' not in content.lower():\n",
    "                    valid_files_count += 1\n",
    "                    valid_files_with_content.append((file_name, content))\n",
    "\n",
    "# Writing the filtered results to an output file\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for file_name, content in valid_files_with_content:\n",
    "        outfile.write(f\"Content before abstract in {file_name}:\\n{content}\\n\\n\")\n",
    "\n",
    "# Print or save the statistics summary\n",
    "# print(f\"Total number of files processed: {total_files_count}\")\n",
    "# print(f\"Total number of files with valid content before abstract: {valid_files_count}\")\n",
    "# print(f\"Filtered output saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1818 files with content\n",
    "183 no abstract found (probably wrong tex file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 1818\n",
      "Number of papers with tag '\\\\affiliations': 53\n",
      "Number of papers with tag '\\\\affiliation': 2269\n",
      "Number of papers with tag '\\\\icmlaffiliation': 85\n",
      "Number of papers with tag '\\\\institute': 97\n",
      "Number of papers with tag '\\\\affil': 37\n",
      "Number of papers with tag '\\\\aff': 3\n",
      "Number of papers with tag '\\\\AFF': 11\n",
      "Number of papers with tag '\\\\address': 261\n",
      "Tagged output files saved in directory: ./tagged_outputs/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Define the input file path and output file directory\n",
    "input_file_path = 'filtered_files_with_content_macro.txt'\n",
    "output_dir = './tagged_outputs/'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tags to search for, their output files, and counters\n",
    "tags_to_search = [\n",
    "    r'\\\\affiliations', r'\\\\affiliation', r'\\\\icmlaffiliation',  \n",
    "    r'\\\\institute', r'\\\\affil', r'\\\\aff', r'\\\\AFF', r'\\\\address'\n",
    "]\n",
    "tag_files = {tag: [] for tag in tags_to_search}  # Dictionary to store file content for each tag\n",
    "tag_counts = {tag: 0 for tag in tags_to_search}\n",
    "\n",
    "# Function to extract the content within the first level of braces after each tag\n",
    "def extract_tag_content(tag, content):\n",
    "    pattern = rf\"({tag}\\{{)\"\n",
    "    results = []\n",
    "    start = 0\n",
    "    while (match := re.search(pattern, content[start:])) is not None:\n",
    "        # Find the opening brace position and keep the tag itself\n",
    "        start_idx = start + match.start()\n",
    "        tag_with_brace = match.group(1)  # Keep the matched tag including the opening brace\n",
    "        brace_level = 1\n",
    "        end_idx = start_idx + len(tag_with_brace)\n",
    "\n",
    "        # Find the matching closing brace\n",
    "        while brace_level > 0 and end_idx < len(content):\n",
    "            if content[end_idx] == '{':\n",
    "                brace_level += 1\n",
    "            elif content[end_idx] == '}':\n",
    "                brace_level -= 1\n",
    "            end_idx += 1\n",
    "\n",
    "        # Extract and store the full tag with content in braces\n",
    "        results.append(content[start_idx:end_idx])\n",
    "        start = end_idx  # Move the start index to continue searching\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and assign content for each tag in a file\n",
    "def extract_and_assign_tag_content(file_name, content, tags):\n",
    "    for tag in tags:\n",
    "        # Extract content with braces for each occurrence of the tag\n",
    "        matches = extract_tag_content(tag, content)\n",
    "\n",
    "        # If matches are found, add them to the corresponding tag's list\n",
    "        if matches:\n",
    "            extracted_content = f\"Content in {tag} for {file_name}:\\n\" + \"\\n\".join(matches) + \"\\n\"\n",
    "            tag_files[tag].append(extracted_content)\n",
    "            tag_counts[tag] += len(matches)  # Count each match found\n",
    "            return True  # Stop after the first matching tag\n",
    "    return False\n",
    "\n",
    "# Process the input file\n",
    "total_files_count = 0\n",
    "\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count += 1\n",
    "            lines = block.split('\\n', 1)\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "\n",
    "                # Extract and assign content within tags\n",
    "                extract_and_assign_tag_content(file_name, content, tags_to_search)\n",
    "\n",
    "# Write each tag's extracted content to its respective output file\n",
    "for tag in tags_to_search:\n",
    "    output_file_path = \"{}{}_output.txt\".format(output_dir, tag.replace('\\\\', ''))\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(tag_files[tag]))\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total number of files processed: {total_files_count}\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"Number of papers with tag '{tag}': {count}\")\n",
    "print(f\"Tagged output files saved in directory: {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
