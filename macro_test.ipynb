{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "import zipfile\n",
    "import importlib\n",
    "import regex as re\n",
    "import pyperclip  \n",
    "import TexSoup as TS\n",
    "from TexSoup.tokens import MATH_ENV_NAMES\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 83\u001b[0m\n\u001b[1;32m     81\u001b[0m latest_versions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Track the latest version of each identifier\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241m.\u001b[39mZipFile(zip_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_file:\n\u001b[1;32m     84\u001b[0m     tar_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m zip_file\u001b[38;5;241m.\u001b[39mnamelist() \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.tar.gz\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tar_name \u001b[38;5;129;01min\u001b[39;00m tar_files:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "def find_doc_class(fp, name_match=False):\n",
    "    '''Search for document class related lines in a file and return a code to represent the type'''\n",
    "    doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "    sub_doc_class = re.compile(r\"^\\s*\\\\document(?:style|class).*(?:\\{standalone\\}|\\{subfiles\\})\")\n",
    "\n",
    "    # Read the content as bytes\n",
    "    file_content = fp.read()\n",
    "    try:\n",
    "        # Try decoding with UTF-8\n",
    "        file_text = file_content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin-1 encoding if UTF-8 fails\n",
    "        file_text = file_content.decode('latin-1')\n",
    "\n",
    "    for line in file_text.splitlines():\n",
    "        if doc_class_pat.search(line):\n",
    "            if name_match:\n",
    "                if sub_doc_class.search(line):\n",
    "                    return -99999\n",
    "                return 1  # Found document class line\n",
    "    return 0  # No document class line found\n",
    "\n",
    "def find_main_tex_source_in_tar(tar_file, encoding='utf-8'):\n",
    "    tex_names = set([\"paper\", \"main\", \"ms.\", \"article\"])\n",
    "    tex_files = [f for f in tar_file.getnames() if f.endswith('.tex')]\n",
    "\n",
    "    if len(tex_files) == 1:\n",
    "        return tex_files[0]\n",
    "\n",
    "    main_files = {}\n",
    "    for tf in tex_files:\n",
    "        depth = len(tf.split('/')) - 1\n",
    "        has_main_name = any(kw in tf for kw in tex_names)\n",
    "        fp = tar_file.extractfile(tf)\n",
    "        if fp:\n",
    "            main_files[tf] = find_doc_class(fp, name_match=has_main_name) - depth\n",
    "            fp.close()\n",
    "\n",
    "    return max(main_files, key=main_files.get) if main_files else None\n",
    "\n",
    "def pre_format(text):\n",
    "    source_text = text.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }').replace(')$', ') $')\n",
    "    return source_text\n",
    "\n",
    "def source_from_tar(tar_file, encoding='utf-8'):\n",
    "    tex_main = find_main_tex_source_in_tar(tar_file, encoding=encoding)\n",
    "    if tex_main:\n",
    "        fp = tar_file.extractfile(tex_main)\n",
    "        if fp is not None:\n",
    "            file_content = fp.read()  # Read as bytes to keep it in memory\n",
    "            try:\n",
    "                # Attempt to decode using UTF-8\n",
    "                source_text = pre_format(file_content.decode(encoding))\n",
    "            except UnicodeDecodeError:\n",
    "                # Fallback to latin-1 encoding if UTF-8 fails\n",
    "                source_text = pre_format(file_content.decode('latin-1'))\n",
    "            return source_text\n",
    "    return None\n",
    "\n",
    "def extract_before_abstract(source_text):\n",
    "    no_comments_text = re.sub(r'(?<!\\\\)%.*', '', source_text)\n",
    "    no_usepackage_text = re.sub(r'\\\\usepackage\\s*\\{[^}]+\\}', '', no_comments_text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', no_usepackage_text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\[[^\\]]*\\]\\{[^}]*\\}', '', no_usepackage_text)\n",
    "    text = re.sub(r'\\$[^$]*\\$', '', no_usepackage_text)\n",
    "    text = no_usepackage_text.replace('{', '').replace('}', '').replace('\\n', ' ')\n",
    "    text = ' '.join(no_usepackage_text.split())\n",
    "    abstract_match = re.search(r'\\\\begin\\s*\\{\\s*abstract\\s*\\}', text)\n",
    "\n",
    "    if abstract_match:\n",
    "        return text[:abstract_match.start()].strip()\n",
    "    \n",
    "    abstract_word_match = re.search(r'\\babstract\\b', text, re.IGNORECASE)\n",
    "    if abstract_word_match:\n",
    "        return text[:abstract_word_match.start()].strip()\n",
    "    return None\n",
    "\n",
    "zip_file_path = \"./2401.zip\"\n",
    "output_path = \"./firstoutput.txt\"\n",
    "\n",
    "latest_versions = {}\n",
    "# Track the latest version of each identifier\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    tar_files = [f for f in zip_file.namelist() if f.endswith('.tar.gz')]\n",
    "    for tar_name in tar_files:\n",
    "        base_name, version = tar_name.rsplit(\"v\", 1)\n",
    "        version_num = int(version.split('.')[0])  # Extract version number\n",
    "        if base_name not in latest_versions or version_num > latest_versions[base_name][1]:\n",
    "            latest_versions[base_name] = (tar_name, version_num)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "    for base_name, (tar_name, version) in latest_versions.items():\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "            with zip_file.open(tar_name) as tar_bytes:\n",
    "                tar_file = tarfile.open(fileobj=io.BytesIO(tar_bytes.read()), mode='r:gz')\n",
    "                source_text = source_from_tar(tar_file)\n",
    "                if source_text:\n",
    "                    content_before_abstract = extract_before_abstract(source_text)\n",
    "                    if content_before_abstract:\n",
    "                        output_file.write(f\"Content before abstract in {tar_name}:\\n{content_before_abstract}\\n\\n\")\n",
    "                    else:\n",
    "                        output_file.write(f\"No abstract found in {tar_name}, or no content before abstract.\\n\\n\")\n",
    "                tar_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input from a text file and filter out files without valid content before the abstract\n",
    "input_file_path = 'firstoutput.txt'  # Replace with your actual file path\n",
    "output_file_path = 'filtered_files_with_content_macro.txt'\n",
    "\n",
    "# Variables to keep track of statistics\n",
    "total_files_count_author = 0\n",
    "valid_files_count = 0\n",
    "valid_files_with_content = []\n",
    "\n",
    "# Reading and processing the input file\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "    \n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count_author += 1\n",
    "            lines = block.split('\\n', 1)  # Split to separate the file name from its content\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                \n",
    "                # Check if the content does not indicate \"No abstract found\"\n",
    "                if 'No abstract found' not in content and 'no content before abstract' not in content.lower():\n",
    "                    valid_files_count += 1\n",
    "                    valid_files_with_content.append((file_name, content))\n",
    "\n",
    "# Writing the filtered results to an output file\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for file_name, content in valid_files_with_content:\n",
    "        outfile.write(f\"Content before abstract in {file_name}:\\n{content}\\n\\n\")\n",
    "\n",
    "# Print or save the statistics summary\n",
    "# print(f\"Total number of files processed: {total_files_count}\")\n",
    "# print(f\"Total number of files with valid content before abstract: {valid_files_count}\")\n",
    "# print(f\"Filtered output saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1818 files with content\n",
    "183 no abstract found (probably wrong tex file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 1818\n",
      "Number of papers with tag '\\\\institution': 104\n",
      "Number of papers with tag '\\\\affiliations': 35\n",
      "Number of papers with tag '\\\\affiliation': 451\n",
      "Number of papers with tag '\\\\icmlaffiliation': 37\n",
      "Number of papers with tag '\\\\institute': 93\n",
      "Number of papers with tag '\\\\affil': 30\n",
      "Number of papers with tag '\\\\aff': 7\n",
      "Number of papers with tag '\\\\AFF': 5\n",
      "Number of papers with tag '\\\\address': 146\n",
      "Tagged output files saved in directory: ./tagged_outputs/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "# Define the input file path and output file directory\n",
    "input_file_path = 'filtered_files_with_content_macro.txt'\n",
    "output_dir = './tagged_outputs/'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Tags to search for, their output files, and counters\n",
    "tags_to_search = [\n",
    "    r'\\\\institution',\n",
    "    r'\\\\affiliations', r'\\\\affiliation', r'\\\\icmlaffiliation',  \n",
    "    r'\\\\institute', r'\\\\affil', r'\\\\aff', r'\\\\AFF', r'\\\\address'\n",
    "]\n",
    "tag_files = {tag: [] for tag in tags_to_search}  # Dictionary to store file content for each tag\n",
    "tag_counts = {tag: 0 for tag in tags_to_search}   # Counter for unique papers containing each tag\n",
    "\n",
    "# Function to extract the content within the first level of braces after each tag\n",
    "def extract_tag_content(tag, content):\n",
    "    pattern = rf\"({tag}\\{{)\"\n",
    "    results = []\n",
    "    start = 0\n",
    "    while (match := re.search(pattern, content[start:])) is not None:\n",
    "        # Find the opening brace position and keep the tag itself\n",
    "        start_idx = start + match.start()\n",
    "        tag_with_brace = match.group(1)  # Keep the matched tag including the opening brace\n",
    "        brace_level = 1\n",
    "        end_idx = start_idx + len(tag_with_brace)\n",
    "\n",
    "        # Find the matching closing brace\n",
    "        while brace_level > 0 and end_idx < len(content):\n",
    "            if content[end_idx] == '{':\n",
    "                brace_level += 1\n",
    "            elif content[end_idx] == '}':\n",
    "                brace_level -= 1\n",
    "            end_idx += 1\n",
    "\n",
    "        # Extract and store the full tag with content in braces\n",
    "        results.append(content[start_idx:end_idx])\n",
    "        start = end_idx  # Move the start index to continue searching\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and assign content for each tag in a file\n",
    "def extract_and_assign_tag_content(file_name, content, tags):\n",
    "    tag_found = set()  # Track tags found in this paper to avoid counting duplicates\n",
    "\n",
    "    for tag in tags:\n",
    "        # Extract content with braces for each occurrence of the tag\n",
    "        matches = extract_tag_content(tag, content)\n",
    "\n",
    "        # If matches are found, add them to the corresponding tag's list\n",
    "        if matches:\n",
    "            extracted_content = f\"Content in {tag} for {file_name}:\\n\" + \"\\n\".join(matches) + \"\\n\"\n",
    "            tag_files[tag].append(extracted_content)\n",
    "\n",
    "            # Only increment the count if this tag hasn't been counted for this paper yet\n",
    "            if tag not in tag_found:\n",
    "                tag_counts[tag] += 1\n",
    "                tag_found.add(tag)  # Mark this tag as found for this paper\n",
    "\n",
    "    return bool(tag_found)  # Return True if any tags were found\n",
    "\n",
    "# Process the input file\n",
    "total_files_count = 0\n",
    "\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count += 1\n",
    "            lines = block.split('\\n', 1)\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "\n",
    "                # Extract and assign content within tags\n",
    "                extract_and_assign_tag_content(file_name, content, tags_to_search)\n",
    "\n",
    "# Write each tag's extracted content to its respective output file\n",
    "for tag in tags_to_search:\n",
    "    output_file_path = \"{}{}_output.txt\".format(output_dir, tag.replace('\\\\', ''))\n",
    "    with open(output_file_path, 'w') as outfile:\n",
    "        outfile.write('\\n'.join(tag_files[tag]))\n",
    "\n",
    "# Print statistics\n",
    "print(f\"Total number of files processed: {total_files_count}\")\n",
    "for tag, count in tag_counts.items():\n",
    "    print(f\"Number of papers with tag '{tag}': {count}\")\n",
    "print(f\"Tagged output files saved in directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:No persistent ROR cache found. Using empty cache.\n",
      "WARNING:root:No exact match found for institution 'Polytechnic School, University of São Paulo' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Also Visiting Fellow at King's College and Research Affiliate at the Department of History and Philosophy of Science, University of Cambridge' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Universitat Polit\\`ecnica de Val\\`encia' in ROR.\n",
      "INFO:root:Found ROR ID for 'Charles University': https://ror.org/024d6js02\n",
      "WARNING:root:No exact match found for institution 'College of Command and Control Engineering, Army Engineering University of PLA' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Continental-NTU Corporate Lab, Nanyang Technological University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'School of Computer Science and Engineering, Nanyang Technological University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Dept. of Information Technology Specialization, FPT University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'University of Waterloo, Canada' in ROR.\n",
      "WARNING:root:No exact match found for institution 'William \\& Mary' in ROR.\n",
      "INFO:root:Found ROR ID for 'Xidian University': https://ror.org/05s92vm98\n",
      "INFO:root:Found ROR ID for 'Nanyang Technological University': https://ror.org/02e7b5302\n",
      "INFO:root:Found ROR ID for 'Nanjing University': https://ror.org/01rxvg760\n",
      "WARNING:root:No exact match found for institution 'Institute of Information Engineering, Chinese Academy of Sciences' in ROR.\n",
      "INFO:root:Found ROR ID for 'Singapore Management University': https://ror.org/050qmg959\n",
      "WARNING:root:No ROR information found for institution 'Featurespace'.\n",
      "INFO:root:Found ROR ID for 'University of Stuttgart': https://ror.org/04vnq7t77\n",
      "INFO:root:Found ROR ID for 'Zhejiang University': https://ror.org/00a2xv884\n",
      "INFO:root:Found ROR ID for 'Aalborg University': https://ror.org/04m5j1k67\n",
      "INFO:root:Found ROR ID for 'Case Western Reserve University': https://ror.org/051fd9666\n",
      "WARNING:root:No ROR information found for institution 'Zilliz'.\n",
      "INFO:root:Found ROR ID for 'Zhejiang Lab': https://ror.org/02m2h7991\n",
      "INFO:root:Found ROR ID for 'Tongji University': https://ror.org/03rc6as71\n",
      "INFO:root:Found ROR ID for 'Hangzhou Dianzi University': https://ror.org/0576gt767\n",
      "INFO:root:Found ROR ID for 'University of Maryland, College Park': https://ror.org/047s2c258\n",
      "WARNING:root:No exact match found for institution 'WeChat Pay, Tencent' in ROR.\n",
      "INFO:root:Found ROR ID for 'King Mongkut's Institute of Technology Ladkrabang': https://ror.org/055mf0v62\n",
      "INFO:root:Found ROR ID for 'National Electronics and Computer Technology Center': https://ror.org/04z82ry91\n",
      "WARNING:root:No exact match found for institution 'University of Michigan' in ROR.\n",
      "INFO:root:Found ROR ID for 'Huazhong University of Science and Technology': https://ror.org/00p991c53\n",
      "WARNING:root:No exact match found for institution 'University of New South Wales' in ROR.\n",
      "INFO:root:Found ROR ID for 'Monash University': https://ror.org/02bfwt286\n",
      "WARNING:root:No exact match found for institution 'IIIT Hyderabad' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Ant Group' in ROR.\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: Toyota Motor Europe NV/SA\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: Sorbonne Universit\\'{e\n",
      "INFO:root:Found ROR ID for 'University of California, Santa Barbara': https://ror.org/02t274463\n",
      "INFO:root:Found ROR ID for 'Tsinghua University': https://ror.org/03cve4549\n",
      "WARNING:root:No exact match found for institution 'Infinigence-AI' in ROR.\n",
      "INFO:root:Found ROR ID for 'Shanghai Jiao Tong University': https://ror.org/0220qvk04\n",
      "WARNING:root:No exact match found for institution 'Department of Engineering Science, University of Oxford' in ROR.\n",
      "INFO:root:Found ROR ID for 'KU Leuven': https://ror.org/05f950310\n",
      "INFO:root:Found ROR ID for 'King's College London': https://ror.org/0220mzb33\n",
      "INFO:root:Found ROR ID for 'Liverpool Hope University': https://ror.org/03ctjbj91\n",
      "WARNING:root:No exact match found for institution '\\& VRAIN, Universitat Politecnica de Valencia' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Lua Health' in ROR.\n",
      "INFO:root:Found ROR ID for 'Massachusetts Institute of Technology': https://ror.org/042nb2s44\n",
      "WARNING:root:No exact match found for institution 'University of California San Diego' in ROR.\n",
      "INFO:root:Found ROR ID for 'Delft University of Technology': https://ror.org/02e2c7k09\n",
      "WARNING:root:No exact match found for institution 'BIBA - Bremer Institut für Produktion und Logistik GmbH' in ROR.\n",
      "INFO:root:Found ROR ID for 'Universidad EAFIT': https://ror.org/03y3y9v44\n",
      "WARNING:root:No ROR information found for institution 'NeuralMind.ai'.\n",
      "WARNING:root:No exact match found for institution 'University of Campinas' in ROR.\n",
      "WARNING:root:No exact match found for institution 'National Center for State Courts (NCSC)' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Brazilian Association of Jurimetrics (ABJ)' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Terranova Consultoria' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Brazilian Federal Court of Accounts (TCU)' in ROR.\n",
      "INFO:root:Found ROR ID for 'New York University': https://ror.org/0190ak572\n",
      "WARNING:root:No exact match found for institution 'Adobe Research' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of California, San Diego': https://ror.org/0168r3w48\n",
      "INFO:root:Found ROR ID for 'Denison University': https://ror.org/05pqx1c24\n",
      "INFO:root:Found ROR ID for 'University of Southern California': https://ror.org/03taz7m60\n",
      "INFO:root:Found ROR ID for 'Georgia Institute of Technology': https://ror.org/01zkghx44\n",
      "WARNING:root:No exact match found for institution 'National Univ. of Defense Technology' in ROR.\n",
      "INFO:root:Found ROR ID for 'Westlake University': https://ror.org/05hfa4n20\n",
      "INFO:root:Found ROR ID for 'National University of Singapore': https://ror.org/01tgyzw49\n",
      "INFO:root:Found ROR ID for 'University of Science and Technology of China': https://ror.org/04c4dkn09\n",
      "INFO:root:Found ROR ID for 'Shandong University': https://ror.org/0207yh398\n",
      "WARNING:root:No exact match found for institution 'State Key Lab of CAD\\&CG, Zhejiang University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'School of Art and Archaeology, Zhejiang University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'The Hong Kong University of Science and Technology' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Microsoft Research Asia' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Alibaba Group' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Shenzhen International Graduate School, Tsinghua University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Università degli studi dell'Aquila' in ROR.\n",
      "WARNING:root:No exact match found for institution 'DIRO, University of Montreal' in ROR.\n",
      "INFO:root:Found ROR ID for 'Carnegie Mellon University': https://ror.org/05x2bcf33\n",
      "WARNING:root:No exact match found for institution 'Google DeepMind' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Google Research' in ROR.\n",
      "INFO:root:Found ROR ID for 'Stanford University': https://ror.org/00f54p054\n",
      "WARNING:root:No exact match found for institution 'Leiden Institute of Advanced Computer Science' in ROR.\n",
      "INFO:root:Found ROR ID for 'Northwestern University': https://ror.org/00m6w7z96\n",
      "INFO:root:Found ROR ID for 'University of Liverpool': https://ror.org/04xs57h96\n",
      "INFO:root:Found ROR ID for 'Xi’an Jiaotong-Liverpool University': https://ror.org/03zmrmn05\n",
      "INFO:root:Found ROR ID for 'Duke Kunshan University': https://ror.org/04sr5ys16\n",
      "INFO:root:Found ROR ID for 'Southeast University': https://ror.org/00cf0ab87\n",
      "INFO:root:Found ROR ID for 'Hamburg University of Technology': https://ror.org/04bs1pb34\n",
      "WARNING:root:No exact match found for institution 'JetBrains Research' in ROR.\n",
      "INFO:root:Found ROR ID for 'University College London': https://ror.org/02jx3x895\n",
      "WARNING:root:No ROR information found for institution 'Privitar'.\n",
      "WARNING:root:No exact match found for institution 'Flower Labs' in ROR.\n",
      "INFO:root:Found ROR ID for 'Cardiff University': https://ror.org/03kk7td41\n",
      "INFO:root:Found ROR ID for 'Virginia Commonwealth University': https://ror.org/02nkdxk79\n",
      "WARNING:root:No exact match found for institution 'College of Information Studies, University of Maryland' in ROR.\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: \\institution{College of Information Studies, University of Maryland\n",
      "WARNING:root:No exact match found for institution 'Department of Mechanical Engineering, University of Maryland' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Electrical \\& Computer Engineering Department, Rice University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Dept of Neurology, Massachusetts General Hospital, Harvard Medical School' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Sleep, Chronobiology, and Health Laboratory, School of Nursing, Oregon Institute of Occupational Health Sciences, Oregon Health \\& Science University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Turner Institute for Brain and Mental Health, School of Psychological Sciences, Monash University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Institute of Automation, CAS' in ROR.\n",
      "WARNING:root:No exact match found for institution 'School of Artificial Intelligence, UCAS' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Department of Computer Science, University of Toronto' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Microsoft Research' in ROR.\n",
      "WARNING:root:No exact match found for institution 'The University of Auckland' in ROR.\n",
      "INFO:root:Found ROR ID for 'Pennsylvania State University': https://ror.org/04p491231\n",
      "WARNING:root:No exact match found for institution 'The University of Queensland' in ROR.\n",
      "INFO:root:Found ROR ID for 'Southern University of Science and Technology': https://ror.org/049tv2d57\n",
      "WARNING:root:No exact match found for institution 'The University of Technology Sydney' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Huawei Noah's Ark Lab' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Université Paris Cité, CNRS, IRIF' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Aix Marseille Univ, CNRS, LIS' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Independent Researcher' in ROR.\n",
      "INFO:root:Found ROR ID for 'Tel Aviv University': https://ror.org/04mhzgx49\n",
      "INFO:root:Found ROR ID for 'University of California, Irvine': https://ror.org/04gyf1771\n",
      "WARNING:root:No exact match found for institution 'Max Planck Institute, Bio-Cybernetics' in ROR.\n",
      "INFO:root:Found ROR ID for 'Hebrew University of Jerusalem': https://ror.org/03qxff017\n",
      "INFO:root:Found ROR ID for 'Bar-Ilan University': https://ror.org/03kgsv495\n",
      "WARNING:root:No exact match found for institution 'KAIST' in ROR.\n",
      "INFO:root:Found ROR ID for 'RMIT University': https://ror.org/04ttjf776\n",
      "WARNING:root:No exact match found for institution 'The University of Melbourne' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Department of Educational \\& Counselling Psychology\\\\ McGill University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'School of Computer Science\\\\ McGill University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'AI Risk and Vulnerability Alliance' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Harbin Institute of Technology, Shenzhen' in ROR.\n",
      "WARNING:root:No exact match found for institution 'The University of Sydney' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of Connecticut': https://ror.org/02der9h97\n",
      "INFO:root:Found ROR ID for 'Washington University in St. Louis': https://ror.org/01yc7t268\n",
      "INFO:root:Found ROR ID for 'San Jose State University': https://ror.org/04qyvz380\n",
      "WARNING:root:No exact match found for institution 'University of Vienna, Faculty of Computer Science, Research Network Data Science, UniVie Doctoral School Computer Science DoCS' in ROR.\n",
      "WARNING:root:No exact match found for institution 'University of Vienna, Faculty of Computer Science' in ROR.\n",
      "WARNING:root:No exact match found for institution 'University of Vienna, Faculty of Computer Science, Research Network Data Science' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of British Columbia': https://ror.org/03rmrcq20\n",
      "INFO:root:Found ROR ID for 'University of Washington': https://ror.org/00cvxb145\n",
      "WARNING:root:No exact match found for institution 'Interdisciplinary Centre for Security, Reliability and Trust, University of' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Branch Business \\& Information Systems Engineering, Fraunhofer FIT' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of Technology Sydney': https://ror.org/03f0f6041\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: $^a$ Sichuan University, $^b$ Tsinghua University, $^c$ Vanderbilt University \\\\ $^d$ Engineering Research Center of Machine Learning and Industry Intelligence, Ministry of Education\n",
      "INFO:root:Found ROR ID for 'Sapienza University of Rome': https://ror.org/02be6w209\n",
      "INFO:root:Found ROR ID for 'Technology Innovation Institute': https://ror.org/001kv2y39\n",
      "INFO:root:Found ROR ID for 'University of Pisa': https://ror.org/03ad39j10\n",
      "INFO:root:Found ROR ID for 'Sun Yat-sen University': https://ror.org/0064kty71\n",
      "INFO:root:Found ROR ID for 'University of Waterloo': https://ror.org/01aff2v68\n",
      "WARNING:root:No exact match found for institution 'Constructor Institute Schaffhausen' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Constructor University Bremen' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of Chicago': https://ror.org/024mw5h28\n",
      "WARNING:root:No exact match found for institution 'Northwestern Univerisity' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Department of Computer Science, University of Vermont' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Vermont Complex Systems Center, University of Vermont' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Department of Mathematics \\& Statistics, University of Vermont' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Department of Art and Art History, University of Vermont' in ROR.\n",
      "INFO:root:Found ROR ID for 'Nanjing University of Science and Technology': https://ror.org/00xp9wg62\n",
      "INFO:root:Found ROR ID for 'Anhui Science and Technology University': https://ror.org/01pn91c28\n",
      "INFO:root:Found ROR ID for 'Nankai University': https://ror.org/01y1kjr75\n",
      "INFO:root:Found ROR ID for 'Beijing Jiaotong University': https://ror.org/01yj56c84\n",
      "WARNING:root:No exact match found for institution 'Tencent' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Noah's Ark Lab, Huawei' in ROR.\n",
      "WARNING:root:No exact match found for institution 'The University of Hong Kong' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Texas A\\&M University' in ROR.\n",
      "INFO:root:Found ROR ID for 'George Mason University': https://ror.org/02jqj7156\n",
      "INFO:root:Found ROR ID for 'Emory University': https://ror.org/03czfpz43\n",
      "WARNING:root:No exact match found for institution 'School of Information Science and Engineering, Shandong Normal University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Key Lab of HCST (PKU), MOE; SCS, Peking University' in ROR.\n",
      "INFO:root:Found ROR ID for 'Yonsei University': https://ror.org/01wjejq96\n",
      "WARNING:root:No exact match found for institution 'TU Darmstadt, hessian.AI' in ROR.\n",
      "WARNING:root:No exact match found for institution 'LMU Munich, MCML' in ROR.\n",
      "WARNING:root:No exact match found for institution 'TU Darmstadt, DFKI, LAION' in ROR.\n",
      "WARNING:root:No exact match found for institution 'TU Darmstadt, hessian.AI, DFKI' in ROR.\n",
      "INFO:root:Found ROR ID for 'Peking University': https://ror.org/02v51f717\n",
      "WARNING:root:No exact match found for institution 'Kuaishou Technology' in ROR.\n",
      "WARNING:root:No ROR information found for institution 'Unaffiliated'.\n",
      "WARNING:root:No exact match found for institution 'DFKI, Saarland Informatics Campus' in ROR.\n",
      "INFO:root:Found ROR ID for 'Renmin University of China': https://ror.org/041pakw92\n",
      "INFO:root:Found ROR ID for 'ETH Zurich': https://ror.org/05a28rw58\n",
      "WARNING:root:No exact match found for institution 'IST Austria and Mysten Labs' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Mysten Labs and University College London (UCL)' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Mysten Labs' in ROR.\n",
      "INFO:root:Found ROR ID for 'North Carolina State University': https://ror.org/04tj63d06\n",
      "INFO:root:Found ROR ID for 'University of Bristol': https://ror.org/0524sp257\n",
      "INFO:root:Found ROR ID for 'Seattle University': https://ror.org/02jqc0m91\n",
      "WARNING:root:No exact match found for institution 'Dataminr' in ROR.\n",
      "INFO:root:Found ROR ID for 'Chongqing University': https://ror.org/023rhb549\n",
      "WARNING:root:No exact match found for institution 'Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education; School of Computer Science, Peking University, Beijing' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Huawei Russian Research Institute' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Institute for Advanced Algorithms Research (Shanghai)' in ROR.\n",
      "WARNING:root:No exact match found for institution '360 AI Research Institute' in ROR.\n",
      "INFO:root:Found ROR ID for 'The University of Tokyo': https://ror.org/057zh3y96\n",
      "INFO:root:Found ROR ID for 'Australian National University': https://ror.org/019wvm592\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: \\textit{Institut f\\\"ur Informatik, Humboldt-Universit\\\"at zu Berlin\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: \\textit{Department of Computer Science, University of York\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: \\textit{Department of Computer Science, University of Brasilia\n",
      "WARNING:root:No exact match found for institution 'No Affiliation' in ROR.\n",
      "INFO:root:Found ROR ID for 'University of Electronic Science and Technology of China': https://ror.org/04qr3zq92\n",
      "INFO:root:Found ROR ID for 'University of Chinese Academy of Sciences': https://ror.org/05qbk4x57\n",
      "WARNING:root:No exact match found for institution 'Hyper Oracle' in ROR.\n",
      "WARNING:root:No exact match found for institution 'Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education \\& School of Computer Science, Peking University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'School of Information Management and Engineering, Shanghai University of Finance and Economics' in ROR.\n",
      "WARNING:root:No exact match found for institution 'National Key Laboratory for Novel Software Technology, Nanjing University \\& School of Artificial Intelligence, Nanjing University' in ROR.\n",
      "WARNING:root:No exact match found for institution 'LIACS, Leiden University' in ROR.\n",
      "INFO:root:Found ROR ID for 'Barcelona Supercomputing Center': https://ror.org/05sd8tv96\n",
      "WARNING:root:No exact match found for institution 'Universitat Polit\\`ecnica de Catalunya' in ROR.\n",
      "INFO:root:Saved ROR cache to file.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configure ROR API information\n",
    "ROR_SEARCH_URL = 'https://api.ror.org/organizations'\n",
    "\n",
    "# Initialize ROR cache\n",
    "ror_cache = {}\n",
    "\n",
    "# Load persistent cache\n",
    "cache_file = 'ror_cache.json'\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        ror_cache = json.load(f)\n",
    "    logging.info(\"Loaded persistent ROR cache.\")\n",
    "else:\n",
    "    logging.info(\"No persistent ROR cache found. Using empty cache.\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
    "def query_ror(institution_name):\n",
    "    \"\"\"\n",
    "    Query ROR information, returning details only if the name exactly matches.\n",
    "    \"\"\"\n",
    "    if institution_name in ror_cache:\n",
    "        return ror_cache[institution_name]\n",
    "    \n",
    "    # ROR API search endpoint\n",
    "    params = {'query': institution_name}\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(ROR_SEARCH_URL, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            if data.get('items'):\n",
    "                # Iterate through all returned items to find an exact name match\n",
    "                for item in data['items']:\n",
    "                    ror_name = item.get('name', '').strip()\n",
    "                    # Compare names, case-insensitive and ignore leading/trailing spaces\n",
    "                    if ror_name.lower() == institution_name.strip().lower():\n",
    "                        ror_cache[institution_name] = {\n",
    "                            'ROR_ID': item.get('id', 'N/A'),\n",
    "                            'Name': ror_name,\n",
    "                            'Country': item.get('country', {}).get('country_name', 'N/A'),\n",
    "                            'Type': ', '.join(item.get('types', []))\n",
    "                        }\n",
    "                        logging.info(f\"Found ROR ID for '{institution_name}': {item.get('id', 'N/A')}\")\n",
    "                        return ror_cache[institution_name]\n",
    "                \n",
    "                # If no exact match found\n",
    "                logging.warning(f\"No exact match found for institution '{institution_name}' in ROR.\")\n",
    "                ror_cache[institution_name] = None\n",
    "                return None\n",
    "            else:\n",
    "                logging.warning(f\"No ROR information found for institution '{institution_name}'.\")\n",
    "                ror_cache[institution_name] = None\n",
    "                return None\n",
    "        else:\n",
    "            logging.error(f\"ROR API query failed with status code: {response.status_code} for institution: {institution_name}\")\n",
    "            ror_cache[institution_name] = None\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ROR API query exception for institution: {institution_name}, Error: {e}\")\n",
    "        ror_cache[institution_name] = None\n",
    "        return None\n",
    "\n",
    "# Save cache to a file after all queries\n",
    "def save_cache():\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ror_cache, f)\n",
    "    logging.info(\"Saved ROR cache to file.\")\n",
    "\n",
    "# Process institution_output.txt to add ROR details\n",
    "input_file_path = 'tagged_outputs/institution_output.txt'\n",
    "output_file_path = 'institution_output_with_ror.txt'\n",
    "\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    current_file = None\n",
    "    for line in infile:\n",
    "        if line.startswith(\"Content in \\\\institution for\"):\n",
    "            # Write header line to output\n",
    "            outfile.write(line)\n",
    "            current_file = line.strip()\n",
    "        elif line.startswith(\"\\\\institution{\"):\n",
    "            # Extract institution name within braces\n",
    "            institution_name = re.search(r'\\\\institution\\{(.*?)\\}', line).group(1).strip()\n",
    "            \n",
    "            # Query ROR API for this institution name\n",
    "            ror_info = query_ror(institution_name)\n",
    "            \n",
    "            # Write the original line to output\n",
    "            outfile.write(line)\n",
    "            \n",
    "            # Write ROR information if available\n",
    "            if ror_info:\n",
    "                ror_details = (\n",
    "                    f\"  - ROR_ID: {ror_info['ROR_ID']}\\n\"\n",
    "                    f\"  - Name: {ror_info['Name']}\\n\"\n",
    "                    f\"  - Country: {ror_info['Country']}\\n\"\n",
    "                    f\"  - Type: {ror_info['Type']}\\n\"\n",
    "                )\n",
    "                outfile.write(ror_details)\n",
    "            else:\n",
    "                outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "        elif line.strip() == '':\n",
    "            # Separate sections for each file\n",
    "            outfile.write(\"\\n\")\n",
    "        else:\n",
    "            # Write any other lines as-is\n",
    "            outfile.write(line)\n",
    "\n",
    "# Save the ROR cache after processing all institutions\n",
    "save_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./tagged_outputs/institution_output_with_ror_matches.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "ror_data_path = '1.34_extracted_ror_data.csv'\n",
    "input_file_path = './tagged_outputs/institution_output.txt'\n",
    "output_file_path = './tagged_outputs/institution_output_with_ror_matches.txt'\n",
    "\n",
    "# Step 1: Load ROR data into a dictionary for quick lookups\n",
    "ror_data = {}\n",
    "\n",
    "with open(ror_data_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ror_id = row['id']\n",
    "        name = row['name']\n",
    "        # aliases = row['aliases'].split(',') if row['aliases'] else []\n",
    "        # Combine name and aliases in a list for each ID\n",
    "        ror_data[ror_id] = {\n",
    "            \"name\": name,\n",
    "            \"all_terms\": [name]\n",
    "        }\n",
    "\n",
    "# Step 2: Process institution_output.txt and find ROR matches for each entry\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    content_blocks = infile.read().split('Content in \\\\institution for')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ignore empty blocks\n",
    "            # Write the header for each block\n",
    "            outfile.write(f\"Content in \\\\institution for{block.strip()}\\n\")\n",
    "\n",
    "            # Extract each \\institution{...} tag's content\n",
    "            affiliations = re.findall(r'(\\\\institution\\{(.*?)\\})', block, re.DOTALL)\n",
    "\n",
    "            # Step 3: For each institution entry, check for ROR matches\n",
    "            for full_tag, affiliation in affiliations:\n",
    "                # Write the original \\institution{...} content\n",
    "                outfile.write(f\"\\n{full_tag}\\n\")\n",
    "\n",
    "                # Track matches for the current institution\n",
    "                affiliation_matches = []\n",
    "\n",
    "                # Check each ROR entry for a match\n",
    "                for ror_id, ror_info in ror_data.items():\n",
    "                    for term in ror_info['all_terms']:\n",
    "                        if term and term.lower() in affiliation.lower():  # Case-insensitive substring match\n",
    "                            affiliation_matches.append(f\"  - ROR_ID: {ror_id}\\n  - Name: {ror_info['name']}\")\n",
    "                            break  # Stop at the first match for this ROR entry\n",
    "\n",
    "                # Append matches or \"No exact match found in ROR\"\n",
    "                if affiliation_matches:\n",
    "                    for match in affiliation_matches:\n",
    "                        outfile.write(f\"{match}\\n\")\n",
    "                else:\n",
    "                    outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "\n",
    "            # Separate each block for readability\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of papers with tag \\institution: 104\n",
    "total number of tag \\institution: 526\n",
    "no exact match found for a tag: 186\n",
    "accuracy: 64%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ./tagged_outputs/affiliation_output_with_ror_matches.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "ror_data_path = '1.34_extracted_ror_data.csv'\n",
    "input_file_path = './tagged_outputs/affiliation_output.txt'\n",
    "output_file_path = './tagged_outputs/affiliation_output_with_ror_matches.txt'\n",
    "\n",
    "# Step 1: Load ROR data into a dictionary for quick lookups\n",
    "ror_data = {}\n",
    "\n",
    "with open(ror_data_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ror_id = row['id']\n",
    "        name = row['name']\n",
    "        # aliases = row['aliases'].split(',') if row['aliases'] else []\n",
    "        # Combine name and aliases in a list for each ID\n",
    "        ror_data[ror_id] = {\n",
    "            \"name\": name,\n",
    "            \"all_terms\": [name]  # Add aliases here if necessary\n",
    "        }\n",
    "\n",
    "# Step 2: Process affiliation_output.txt and find ROR matches for each entry\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    content_blocks = infile.read().split('Content in \\\\affiliation for')  # Correctly split on \"affiliation\"\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ignore empty blocks\n",
    "            # Write the header for each block\n",
    "            outfile.write(f\"Content in \\\\affiliation for{block.strip()}\\n\")  # Correctly label as \"affiliation\"\n",
    "\n",
    "            # Extract each \\affiliation{...} tag's content\n",
    "            affiliations = re.findall(r'(\\\\affiliation\\{(.*?)\\})', block, re.DOTALL)\n",
    "\n",
    "            # Step 3: For each affiliation entry, check for ROR matches\n",
    "            for full_tag, affiliation in affiliations:\n",
    "                # Write the original \\affiliation{...} content\n",
    "                outfile.write(f\"\\n{full_tag}\\n\")\n",
    "\n",
    "                # Track matches for the current affiliation\n",
    "                affiliation_matches = []\n",
    "\n",
    "                # Check each ROR entry for a match\n",
    "                for ror_id, ror_info in ror_data.items():\n",
    "                    for term in ror_info['all_terms']:\n",
    "                        if term and term.lower() in affiliation.lower():  # Case-insensitive substring match\n",
    "                            affiliation_matches.append(f\"  - ROR_ID: {ror_id}\\n  - Name: {ror_info['name']}\")\n",
    "                            break  # Stop at the first match for this ROR entry\n",
    "\n",
    "                # Append matches or \"No exact match found in ROR\"\n",
    "                if affiliation_matches:\n",
    "                    for match in affiliation_matches:\n",
    "                        outfile.write(f\"{match}\\n\")\n",
    "                else:\n",
    "                    outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "\n",
    "            # Separate each block for readability\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no exact match: 628\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "ror_data_path = '1.34_extracted_ror_data.csv'\n",
    "input_file_path = './tagged_outputs/address_output.txt'\n",
    "output_file_path = './tagged_outputs/address_output_with_ror_matches.txt'\n",
    "\n",
    "# Step 1: Load ROR data into a dictionary for quick lookups\n",
    "ror_data = {}\n",
    "\n",
    "with open(ror_data_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ror_id = row['id']\n",
    "        name = row['name']\n",
    "        # aliases = row['aliases'].split(',') if row['aliases'] else []\n",
    "        # Combine name and aliases in a list for each ID\n",
    "        ror_data[ror_id] = {\n",
    "            \"name\": name,\n",
    "            \"all_terms\": [name]\n",
    "        }\n",
    "\n",
    "# Step 2: Process institution_output.txt and find ROR matches for each entry\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    content_blocks = infile.read().split('Content in \\\\institution for')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ignore empty blocks\n",
    "            # Write the header for each block\n",
    "            outfile.write(f\"Content in \\\\institution for{block.strip()}\\n\")\n",
    "\n",
    "            # Extract each \\institution{...} tag's content\n",
    "            affiliations = re.findall(r'(\\\\affiliation\\{(.*?)\\})', block, re.DOTALL)\n",
    "\n",
    "            # Step 3: For each institution entry, check for ROR matches\n",
    "            for full_tag, affiliation in affiliations:\n",
    "                # Write the original \\institution{...} content\n",
    "                outfile.write(f\"\\n{full_tag}\\n\")\n",
    "\n",
    "                # Track matches for the current institution\n",
    "                affiliation_matches = []\n",
    "\n",
    "                # Check each ROR entry for a match\n",
    "                for ror_id, ror_info in ror_data.items():\n",
    "                    for term in ror_info['all_terms']:\n",
    "                        if term and term.lower() in affiliation.lower():  # Case-insensitive substring match\n",
    "                            affiliation_matches.append(f\"  - ROR_ID: {ror_id}\\n  - Name: {ror_info['name']}\")\n",
    "                            break  # Stop at the first match for this ROR entry\n",
    "\n",
    "                # Append matches or \"No exact match found in ROR\"\n",
    "                if affiliation_matches:\n",
    "                    for match in affiliation_matches:\n",
    "                        outfile.write(f\"{match}\\n\")\n",
    "                else:\n",
    "                    outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "\n",
    "            # Separate each block for readability\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no exa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Check each normalized ROR name for matches\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ror_name, (ror_id, original_name) \u001b[38;5;129;01min\u001b[39;00m ror_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 57\u001b[0m     ror_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(ror_name\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# Match only if all words in the ROR name are found in the affiliation\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ror_words\u001b[38;5;241m.\u001b[39missubset(affiliation_words) \u001b[38;5;129;01mand\u001b[39;00m ror_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m matched_ror_ids:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "ror_data_path = '1.34_extracted_ror_data.csv'\n",
    "input_file_path = './tagged_outputs/affiliation_output.txt'\n",
    "output_file_path = './tagged_outputs/affiliation_output_with_ror_matches.txt'\n",
    "\n",
    "# Helper function for consistent text normalization\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase, removing punctuation, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all non-alphanumeric and non-space characters\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Step 1: Load ROR data into a dictionary for fast lookups\n",
    "ror_dict = {}\n",
    "\n",
    "with open(ror_data_path, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        ror_id = row['id']\n",
    "        name = row['name']\n",
    "        normalized_name = normalize_text(name)\n",
    "\n",
    "        # Skip single-word institution names\n",
    "        if len(normalized_name.split()) > 1:  # Only consider multi-word names\n",
    "            ror_dict[normalized_name] = (ror_id, name)  # Map normalized name to its ID and original name\n",
    "\n",
    "# Step 2: Process affiliation_output.txt and find ROR matches for each entry\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    content_blocks = infile.read().split('Content in \\\\affiliation for')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ignore empty blocks\n",
    "            # Write the header for each block\n",
    "            outfile.write(f\"Content in \\\\affiliation for{block.strip()}\\n\")\n",
    "\n",
    "            # Extract each \\affiliation{...} tag's content\n",
    "            affiliations = re.findall(r'(\\\\affiliation\\{(.*?)\\})', block, re.DOTALL)\n",
    "\n",
    "            # Step 3: For each affiliation entry, check for ROR matches\n",
    "            for full_tag, affiliation in affiliations:\n",
    "                # Write the original \\affiliation{...} content\n",
    "                outfile.write(f\"\\n{full_tag}\\n\")\n",
    "\n",
    "                # Normalize the affiliation text for comparison\n",
    "                normalized_affiliation = normalize_text(affiliation)\n",
    "                affiliation_words = set(normalized_affiliation.split())\n",
    "\n",
    "                # Track matches for the current affiliation\n",
    "                matched_ror_ids = set()  # Ensure no duplicate matches for the same affiliation\n",
    "                affiliation_matches = []\n",
    "\n",
    "                # Check each normalized ROR name for matches\n",
    "                for ror_name, (ror_id, original_name) in ror_dict.items():\n",
    "                    ror_words = set(ror_name.split())\n",
    "\n",
    "                    # Match only if all words in the ROR name are found in the affiliation\n",
    "                    if ror_words.issubset(affiliation_words) and ror_id not in matched_ror_ids:\n",
    "                        affiliation_matches.append(f\"  - ROR_ID: {ror_id}\\n  - Name: {original_name}\")\n",
    "                        matched_ror_ids.add(ror_id)  # Mark this ROR ID as matched\n",
    "\n",
    "                # Append matches or \"No exact match found in ROR\"\n",
    "                if affiliation_matches:\n",
    "                    for match in affiliation_matches:\n",
    "                        outfile.write(f\"{match}\\n\")\n",
    "                else:\n",
    "                    outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "\n",
    "            # Separate each block for readability\n",
    "            outfile.write(\"\\n\")\n",
    "\n",
    "print(f\"Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Trie' from 'compare' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/compare.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcompare\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trie\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define file paths\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ror_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.34_extracted_ror_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Trie' from 'compare' (/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/compare.py)"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "from compare import Trie\n",
    "\n",
    "# Define file paths\n",
    "ror_data_path = '1.34_extracted_ror_data.csv'\n",
    "common_words_path = 'common english word.txt'\n",
    "input_file_path = './tagged_outputs/affiliation_output.txt'\n",
    "output_file_path = './tagged_outputs/affiliation_output_with_ror_matches_trie.txt'\n",
    "\n",
    "# Helper function for consistent text normalization\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text by converting to lowercase, removing punctuation, and extra spaces.\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove all non-alphanumeric and non-space characters\n",
    "    return text.lower().strip()\n",
    "\n",
    "# Initialize Tries\n",
    "ror_trie = Trie()\n",
    "common_words_trie = Trie()\n",
    "\n",
    "# Step 1: Load common words into Trie for filtering\n",
    "def load_common_words(common_words_path):\n",
    "    with open(common_words_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            word, _ = line.split()\n",
    "            common_words_trie.insert(word.upper())\n",
    "\n",
    "# Step 2: Load ROR data into Trie\n",
    "def load_ror_data(ror_data_path):\n",
    "    ror_dict = {}\n",
    "    with open(ror_data_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            ror_id = row['id']\n",
    "            name = row['name']\n",
    "            aliases = row['aliases'].split(';') if row['aliases'] else []\n",
    "\n",
    "            # Add official name and aliases to the Trie\n",
    "            names = [name] + aliases\n",
    "            for institution_name in names:\n",
    "                normalized_name = normalize_text(institution_name).upper()\n",
    "                if len(normalized_name.split()) > 1:  # Skip single-word names\n",
    "                    ror_trie.insert(normalized_name, ror_id)\n",
    "                    ror_dict[normalized_name] = (ror_id, institution_name)\n",
    "    return ror_dict\n",
    "\n",
    "# Step 3: Match affiliations against ROR Trie\n",
    "def match_affiliations(input_file_path, ror_dict):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        content_blocks = infile.read().split('Content in \\\\affiliation for')\n",
    "\n",
    "        for block in content_blocks:\n",
    "            if block.strip():  # Ignore empty blocks\n",
    "                # Write the header for each block\n",
    "                outfile.write(f\"Content in \\\\affiliation for{block.strip()}\\n\")\n",
    "\n",
    "                # Extract each \\affiliation{...} tag's content\n",
    "                affiliations = re.findall(r'(\\\\affiliation\\{(.*?)\\})', block, re.DOTALL)\n",
    "\n",
    "                # Step 4: Match each affiliation against the Trie\n",
    "                for full_tag, affiliation in affiliations:\n",
    "                    # Write the original \\affiliation{...} content\n",
    "                    outfile.write(f\"\\n{full_tag}\\n\")\n",
    "\n",
    "                    # Normalize the affiliation text for comparison\n",
    "                    normalized_affiliation = normalize_text(affiliation).upper()\n",
    "\n",
    "                    # Match words against ROR Trie\n",
    "                    matched_ids = set()\n",
    "                    words = normalized_affiliation.split()\n",
    "                    for i in range(len(words)):\n",
    "                        for j in range(i + 1, len(words) + 1):\n",
    "                            phrase = \" \".join(words[i:j])\n",
    "                            if common_words_trie.search(phrase):  # Skip common English words\n",
    "                                continue\n",
    "                            match = ror_trie.search(phrase)\n",
    "                            if match and match.is_word:\n",
    "                                matched_ids.update(match.matchedIds)\n",
    "\n",
    "                    # Append matches or \"No exact match found in ROR\"\n",
    "                    if matched_ids:\n",
    "                        for ror_id in matched_ids:\n",
    "                            _, institution_name = ror_dict.get(ror_id, (\"Unknown\", \"Unknown\"))\n",
    "                            outfile.write(f\"  - ROR_ID: {ror_id}\\n  - Name: {institution_name}\\n\")\n",
    "                    else:\n",
    "                        outfile.write(\"  - No exact match found in ROR.\\n\")\n",
    "\n",
    "                # Separate each block for readability\n",
    "                outfile.write(\"\\n\")\n",
    "\n",
    "# Step 4: Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Initializing common words and ROR data...\")\n",
    "    load_common_words(common_words_path)\n",
    "    ror_dict = load_ror_data(ror_data_path)\n",
    "\n",
    "    print(\"Matching affiliations...\")\n",
    "    match_affiliations(input_file_path, ror_dict)\n",
    "\n",
    "    print(f\"Results saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
