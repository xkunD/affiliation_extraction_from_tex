{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "import zipfile\n",
    "import importlib\n",
    "import regex as re\n",
    "import pyperclip  \n",
    "import TexSoup as TS\n",
    "from TexSoup.tokens import MATH_ENV_NAMES\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "333 cLoad the CSV file\n",
    "csv_path = \"data/2201.00_scopus_931_with_ror.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import re\n",
    "import io\n",
    "\n",
    "def find_doc_class(fp, name_match=False):\n",
    "    \"\"\"Search for document class related lines in a file and return a code to represent the type\"\"\"\n",
    "    doc_class_pat = re.compile(r\"^\\s*\\\\document(?:style|class)\")\n",
    "    sub_doc_class = re.compile(r\"^\\s*\\\\document(?:style|class).*(?:\\{standalone\\}|\\{subfiles\\})\")\n",
    "\n",
    "    file_content = fp.read()\n",
    "    try:\n",
    "        file_text = file_content.decode('utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        file_text = file_content.decode('latin-1')\n",
    "\n",
    "    for line in file_text.splitlines():\n",
    "        if doc_class_pat.search(line):\n",
    "            if name_match:\n",
    "                if sub_doc_class.search(line):\n",
    "                    return -99999\n",
    "                return 1  # Found document class line\n",
    "    return 0  # No document class line found\n",
    "\n",
    "def find_main_tex_source_in_folder(zip_file, folder_name):\n",
    "    \"\"\"Find the main .tex file inside a folder within the zip archive\"\"\"\n",
    "    tex_names = {\"paper\", \"main\", \"ms.\", \"article\"}\n",
    "    \n",
    "    # Get all .tex files in the folder\n",
    "    tex_files = [f for f in zip_file.namelist() if f.startswith(folder_name + '/') and f.endswith('.tex')]\n",
    "\n",
    "    if len(tex_files) == 1:\n",
    "        return tex_files[0]  # If there's only one .tex file, return it\n",
    "\n",
    "    main_files = {}\n",
    "    for tex_file in tex_files:\n",
    "        depth = tex_file.count('/') - folder_name.count('/')  # Depth relative to folder\n",
    "        has_main_name = any(kw in tex_file for kw in tex_names)\n",
    "        \n",
    "        with zip_file.open(tex_file) as fp:\n",
    "            main_files[tex_file] = find_doc_class(fp, name_match=has_main_name) - depth\n",
    "\n",
    "    return max(main_files, key=main_files.get) if main_files else None\n",
    "\n",
    "def pre_format(text):\n",
    "    \"\"\"Format LaTeX text by adding spaces where necessary\"\"\"\n",
    "    # return text.replace('\\\\}\\\\', '\\\\} \\\\').replace(')}', ') }').replace(')$', ') $')\n",
    "    return text\n",
    "\n",
    "def source_from_zip(zip_file, folder_name):\n",
    "    \"\"\"Extract and decode the main .tex file from a folder inside the zip archive\"\"\"\n",
    "    tex_main = find_main_tex_source_in_folder(zip_file, folder_name)\n",
    "    if tex_main:\n",
    "        with zip_file.open(tex_main) as fp:\n",
    "            file_content = fp.read()\n",
    "            try:\n",
    "                source_text = pre_format(file_content.decode('utf-8'))\n",
    "            except UnicodeDecodeError:\n",
    "                source_text = pre_format(file_content.decode('latin-1'))\n",
    "            return source_text\n",
    "    return None\n",
    "\n",
    "def extract_before_abstract(source_text):\n",
    "    \"\"\"Extract text before the abstract section\"\"\"\n",
    "    no_comments_text = re.sub(r'(?<!\\\\)%.*', '', source_text)  # Remove comments\n",
    "    # no_usepackage_text = re.sub(r'\\\\usepackage\\s*\\{[^}]+\\}', '', no_comments_text)  # Remove usepackage\n",
    "    # text = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', no_usepackage_text)  # Remove LaTeX commands\n",
    "    text = no_comments_text\n",
    "    # text = re.sub(r'\\\\[a-zA-Z]+\\[[^\\]]*\\]\\{[^}]*\\}', '', text)\n",
    "    # text = re.sub(r'\\$[^$]*\\$', '', text)  # Remove inline math\n",
    "    # text = text.replace('{', '').replace('}', '').replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # output_file.write(f\"text: {text}\\n\")\n",
    "\n",
    "    abstract_word_match = re.search(r'begin{abstract}', text, re.IGNORECASE)\n",
    "    if abstract_word_match:\n",
    "        return text[:abstract_word_match.start()].strip()\n",
    "\n",
    "    abstract_match = re.search(r'abstract{', text)\n",
    "    if abstract_match:\n",
    "        return text[:abstract_match.start()].strip()\n",
    "    \n",
    "    nomacro_abstract_match_2 = re.search(r'abstract', text)\n",
    "    if nomacro_abstract_match_2:\n",
    "        return text[:nomacro_abstract_match_2.start()].strip()\n",
    "    \n",
    "    nomacro_abstract_match_1 = re.search(r'Abstract', text)\n",
    "    if nomacro_abstract_match_1:\n",
    "        return text[:nomacro_abstract_match_1.start()].strip()\n",
    "    \n",
    "    return None\n",
    "\n",
    "zip_file_path = \"./2311_tex_test.zip\"\n",
    "output_path = \"./output.txt\"\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "    base_folder = zip_file.namelist()[0]\n",
    "    folders = {name.rstrip('/') for name in zip_file.namelist() if name.startswith(base_folder) and name.endswith('/') and name != base_folder}  # Get folder names\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for folder in folders:\n",
    "            source_text = source_from_zip(zip_file, folder)\n",
    "            # output_file.write(f\"{folder}:source text: {source_text}\\n\")\n",
    "            if source_text:\n",
    "                content_before_abstract = extract_before_abstract(source_text)\n",
    "                if content_before_abstract:\n",
    "                    output_file.write(f\"Content before abstract in {folder}:\\n{content_before_abstract}\\n\\n\")\n",
    "                else:\n",
    "                    output_file.write(f\"No abstract found in {folder}, or no content before abstract.\\n\\n\")\n",
    "            else:\n",
    "                output_file.write(f\"No source found in {folder}.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 125\n",
      "Total number of files with valid content before abstract: 111\n",
      "Filtered output saved in: filtered_files_with_content_macro.txt\n"
     ]
    }
   ],
   "source": [
    "# Read input from a text file and filter out files without valid content before the abstract\n",
    "input_file_path = 'output.txt'  # Replace with your actual file path\n",
    "output_file_path = 'filtered_files_with_content_macro.txt'\n",
    "\n",
    "# Variables to keep track of statistics\n",
    "total_files_count_author = 0\n",
    "valid_files_count = 0\n",
    "valid_files_with_content = []\n",
    "\n",
    "# Reading and processing the input file\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "    \n",
    "    for block in content_blocks:\n",
    "        if block.strip():  # Ensure we are not processing an empty block\n",
    "            total_files_count_author += 1\n",
    "            lines = block.split('\\n', 1)  # Split to separate the file name from its content\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                \n",
    "                # Check if the content does not indicate \"No abstract found\"\n",
    "                if 'No abstract found' not in content and 'no content before abstract' not in content.lower():\n",
    "                    valid_files_count += 1\n",
    "                    valid_files_with_content.append((file_name, content))\n",
    "\n",
    "# Writing the filtered results to an output file\n",
    "with open(output_file_path, 'w') as outfile:\n",
    "    for file_name, content in valid_files_with_content:\n",
    "        outfile.write(f\"Content before abstract in {file_name}:\\n{content}\\n\\n\")\n",
    "\n",
    "# Print or save the statistics summary\n",
    "print(f\"Total number of files processed: {total_files_count_author}\")\n",
    "print(f\"Total number of files with valid content before abstract: {valid_files_count}\")\n",
    "print(f\"Filtered output saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files processed: 111\n",
      "Extracted tag data saved in: ./tagged_outputs/all_extracted_tags_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json  # For structured storage\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = 'filtered_files_with_content_macro.txt'\n",
    "output_file_path = './tagged_outputs/all_extracted_tags_cleaned.txt'  # Single output file\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
    "\n",
    "# Tags to search for (without leading backslashes for dictionary keys)\n",
    "tags_to_search = [\n",
    "    'orgname','institution', 'affiliations', 'affiliation', 'icmlaffiliation',\n",
    "    'institute', 'affil', 'aff', 'AFF', 'university', 'address'\n",
    "]\n",
    "\n",
    "# Dictionary to store extracted data\n",
    "extracted_data = {}\n",
    "\n",
    "# Function to extract content inside the first level of braces `{}` (handling optional `[]`)\n",
    "def extract_tag_content(tag, content):\n",
    "    pattern = rf\"(\\\\{tag})(\\[[^\\]]*\\])?\\{{\"\n",
    "    results = []\n",
    "    start = 0\n",
    "\n",
    "    while (match := re.search(pattern, content[start:])) is not None:\n",
    "        start_idx = start + match.end()  # Start after the macro (past `{`)\n",
    "        brace_level = 1\n",
    "        end_idx = start_idx\n",
    "\n",
    "        # Find the matching closing brace\n",
    "        while brace_level > 0 and end_idx < len(content):\n",
    "            if content[end_idx] == '{':\n",
    "                brace_level += 1\n",
    "            elif content[end_idx] == '}':\n",
    "                brace_level -= 1\n",
    "            end_idx += 1\n",
    "\n",
    "        # Extract only the content inside the first `{}` (excluding the macro name)\n",
    "        extracted_content = content[start_idx:end_idx - 1].strip()  # Remove trailing `}`\n",
    "        results.append(extracted_content)\n",
    "        start = end_idx  # Move to next occurrence\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to process and store extracted content\n",
    "def extract_and_store_tags(file_name, content, tags):\n",
    "    extracted_data[file_name] = {}  # Initialize storage for this file\n",
    "    found_any_tag = False  # Flag to check if we found any tag\n",
    "\n",
    "    for tag in tags:\n",
    "        matches = extract_tag_content(tag, content)\n",
    "\n",
    "        if matches:\n",
    "            extracted_data[file_name][tag] = matches  # Store under cleaned key\n",
    "            found_any_tag = True  # At least one macro was found\n",
    "            break\n",
    "\n",
    "    # If no macros were found, store the whole pre-abstract content\n",
    "    if not found_any_tag:\n",
    "        extracted_data[file_name][\"full_content\"] = content\n",
    "\n",
    "# Process input file\n",
    "total_files_count = 0\n",
    "\n",
    "with open(input_file_path, 'r') as infile:\n",
    "    content_blocks = infile.read().split('Content before abstract in ')\n",
    "\n",
    "    for block in content_blocks:\n",
    "        if block.strip():\n",
    "            total_files_count += 1\n",
    "            lines = block.split('\\n', 1)\n",
    "\n",
    "            if len(lines) > 1:\n",
    "                file_name = lines[0].strip().replace(':', '')\n",
    "                content = lines[1].strip()\n",
    "                extract_and_store_tags(file_name, content, tags_to_search)\n",
    "\n",
    "json_output = json.dumps(extracted_data, indent=4, ensure_ascii=False)\n",
    "# json_output = json_output.replace(\"\\\\\\\\\", \"\\\\\")  # Convert double backslashes to single\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    outfile.write(json_output)\n",
    "\n",
    "\n",
    "# Print summary\n",
    "print(f\"Total number of files processed: {total_files_count}\")\n",
    "print(f\"Extracted tag data saved in: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded persistent ROR cache.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Configure ROR API information\n",
    "ROR_SEARCH_URL = 'https://api.ror.org/organizations'\n",
    "\n",
    "# Initialize ROR cache\n",
    "ror_cache = {}\n",
    "\n",
    "# Load persistent cache\n",
    "cache_file = 'ror_cache.json'\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "        ror_cache = json.load(f)\n",
    "    logging.info(\"Loaded persistent ROR cache.\")\n",
    "else:\n",
    "    logging.info(\"No persistent ROR cache found. Using empty cache.\")\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
    "def query_ror(institution_name):\n",
    "    \"\"\"\n",
    "    Query ROR information, returning best match from top 3 candidates.\n",
    "    \"\"\"\n",
    "    if institution_name in ror_cache:\n",
    "        return ror_cache[institution_name]\n",
    "\n",
    "    params = {'query': institution_name}\n",
    "\n",
    "    try:\n",
    "        response = requests.get(ROR_SEARCH_URL, params=params, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                logging.warning(f\"No ROR information found for '{institution_name}'.\")\n",
    "                ror_cache[institution_name] = None\n",
    "                return None\n",
    "\n",
    "            # Only consider top 3 results\n",
    "            candidates = items[:3]\n",
    "            chosen = None\n",
    "\n",
    "            logging.info(f\"🔍 Query: {institution_name}\")\n",
    "            for idx, item in enumerate(candidates):\n",
    "                candidate_name = item.get('name', '').lower()\n",
    "                if candidate_name in institution_name.lower():\n",
    "                    chosen = item\n",
    "                    logging.info(f\"✅ Match found with candidate {idx+1}: {item.get('name')}\")\n",
    "                    break\n",
    "                else:\n",
    "                    logging.info(f\"❌ No match for candidate {idx+1}: {item.get('name')}\")\n",
    "\n",
    "            if not chosen:\n",
    "                # Fallback: take first one\n",
    "                chosen = candidates[0]\n",
    "                logging.info(f\"⚠️ No match found, fallback to first candidate: {chosen.get('name')}\")\n",
    "\n",
    "            ror_cache[institution_name] = {\n",
    "                'ROR_ID': chosen.get('id', 'N/A'),\n",
    "                'Name': chosen.get('name', 'N/A'),\n",
    "                'Country': chosen.get('country', {}).get('country_name', 'N/A'),\n",
    "                'Type': ', '.join(chosen.get('types', []))\n",
    "            }\n",
    "            return ror_cache[institution_name]\n",
    "\n",
    "        else:\n",
    "            logging.error(f\"ROR API query failed with status code: {response.status_code} for institution: {institution_name}\")\n",
    "            ror_cache[institution_name] = None\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"ROR API query exception for '{institution_name}', Error: {e}\")\n",
    "        ror_cache[institution_name] = None\n",
    "        return None\n",
    "\n",
    "# Save cache to a file after all queries\n",
    "def save_cache():\n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(ror_cache, f)\n",
    "    logging.info(\"Saved ROR cache to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON successfully loaded!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from extractors.trie_extractor import TrieExtractor\n",
    "from utils.file_reader import read_file\n",
    "\n",
    "# Define file paths\n",
    "input_file_path = 'tagged_outputs/all_extracted_tags_cleaned.txt'\n",
    "output_file_path = 'institution_output_with_ror.json'\n",
    "\n",
    "# Read the file and replace invalid escape sequences\n",
    "with open(input_file_path, 'r', encoding='utf-8') as infile:\n",
    "    raw_data = infile.read()\n",
    "\n",
    "# Fix invalid escape sequences by double-escaping backslashes\n",
    "# fixed_data = raw_data.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "fixed_data = raw_data\n",
    "\n",
    "# Try loading again\n",
    "try:\n",
    "    extracted_data = json.loads(fixed_data)\n",
    "    print(\"✅ JSON successfully loaded!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"❌ JSON loading failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TrieExtractor for full content/address processing\n",
    "extractor = TrieExtractor(\n",
    "    data_path=\"data/1.34_extracted_ror_data.csv\",\n",
    "    common_words_path=\"data/common_english_words.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import spacy\n",
    "# Load the spaCy model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " $^1$ Robotics and Artificial Intelligence Group,  Electrical and Space Engineering, Lule\u0007a \\,\\, University of Technology, Sweden (e-mail: (vissan, sumsat, geonik)@ltu.se)\n"
     ]
    }
   ],
   "source": [
    "abname = ' $^1$ Robotics and Artificial Intelligence Group, Department of Computer Science, Electrical and Space Engineering, Lule\\aa \\,\\, University of Technology, Sweden (e-mail: (vissan, sumsat, geonik)@ltu.se)'\n",
    "abname = re.sub(r'\\b(Faculty of|Department of|School of|College of)\\b[^,]*,', '', abname, flags=re.IGNORECASE)\n",
    "print(abname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^*Physics Department, Arizona State University, Tempe, Arizona 85287, USA.\n",
      " ^†Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\n"
     ]
    }
   ],
   "source": [
    "stringa = \"$^*$Physics Department, Arizona State University, Tempe, Arizona 85287, USA.\\\\\\\\ $^\\\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\"\n",
    "print(latex_to_unicode(stringa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original input:\n",
      "$^*$Physics Department, Arizona State University, Tempe, Arizona 85287, USA. \\\\ $^\\\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\n",
      "\n",
      "Split affiliations before unicode:\n",
      "Institution 1: $Physics Department, Arizona State University, Tempe, Arizona 85287, USA. \\\\ $^\\\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\n"
     ]
    }
   ],
   "source": [
    "def remove_address_spacy(text):\n",
    "    \"\"\"Uses spaCy NER to remove location entities from an institution name\"\"\"\n",
    "    doc = nlp(text)\n",
    "    cleaned_parts = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"}:  # GPE = Geo-Political Entity (City, Country, etc.)\n",
    "            continue  # Skip locations\n",
    "        cleaned_parts.append(ent.text)\n",
    "\n",
    "    return \" \".join(cleaned_parts)\n",
    "\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "converter = LatexNodes2Text()\n",
    "\n",
    "def latex_to_unicode(text):\n",
    "    \"\"\"Safely converts LaTeX special characters to Unicode, handling errors.\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"\"  # Avoid empty input issues\n",
    "    \n",
    "    try:\n",
    "        return converter.latex_to_text(text)\n",
    "    except IndexError as e:\n",
    "        print(f\"⚠️ Warning: Error parsing LaTeX in text: {text}\")\n",
    "        print(\"\\n⚠️ latex_to_unicode parse error encountered!\")\n",
    "        print(\"Full input string:\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(text)\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Error message: {e}\\n\")\n",
    "        return text  # fallback: just return the original text\n",
    "    \n",
    "import re\n",
    "\n",
    "def mild_clean_affiliation(affiliation):\n",
    "    parts = [p.strip() for p in affiliation.split(',')]\n",
    "    if len(parts) > 3:\n",
    "        # If any part contains 'university', return that part + next part (if exists)\n",
    "        for i, part in enumerate(parts):\n",
    "            if 'university' in part.lower():\n",
    "                if i < len(parts) - 1:\n",
    "                    return f\"{part}, {parts[i+1]}\"\n",
    "                else:\n",
    "                    return part\n",
    "        affiliation = ', '.join(parts[:3])\n",
    "    return affiliation.strip()\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "def split_affiliations_before_unicode(raw_text):\n",
    "    pattern = re.compile(r'(\\$\\\\?\\^(?:\\\\dag|\\\\star|\\*|\\d+))')\n",
    "    positions = [m.start() for m in pattern.finditer(raw_text)] + [len(raw_text)]\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(positions) - 1):\n",
    "        segment = raw_text[positions[i]:positions[i+1]].strip()\n",
    "        cleaned_segment = pattern.sub('', segment, count=1).strip()\n",
    "        if cleaned_segment:\n",
    "            results.append(cleaned_segment)\n",
    "\n",
    "    return results\n",
    "\n",
    "stringa = r\"$^*$Physics Department, Arizona State University, Tempe, Arizona 85287, USA. \\\\ $^\\\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\"\n",
    "\n",
    "print(\"\\nOriginal input:\")\n",
    "print(stringa)\n",
    "\n",
    "split_result = split_affiliations_before_unicode(stringa)\n",
    "\n",
    "print(\"\\nSplit affiliations before unicode:\")\n",
    "for idx, aff in enumerate(split_result, 1):\n",
    "    print(f\"Institution {idx}: {aff}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original input:\n",
      "$^1$ Robotics and Artificial Intelligence Group, Department of Computer Science, Electrical and Space Engineering, Lule\\\\aa \\,\\, University of Technology, Sweden (e-mail: (vissan, sumsat, geonik)@ltu.se) \\\\ $^2$ Department of Computer Science, University of Helsinki, Finland.\n",
      "\n",
      "Split affiliations before unicode:\n",
      "Institution 1: $^1$ Robotics and Artificial Intelligence Group, Department of Computer Science, Electrical and Space Engineering, Lule\\\\aa \\,\\, University of Technology, Sweden (e-mail: (vissan, sumsat, geonik)@ltu.se) \\\\ $^2$ Department of Computer Science, University of Helsinki, Finland\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pylatexenc.latex2text import LatexNodes2Text\n",
    "\n",
    "converter = LatexNodes2Text()\n",
    "\n",
    "def remove_address_spacy(text):\n",
    "    \"\"\"Uses spaCy NER to remove location entities from an institution name\"\"\"\n",
    "    doc = nlp(text)\n",
    "    cleaned_parts = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {\"GPE\", \"LOC\"}:\n",
    "            continue  # Skip locations\n",
    "        cleaned_parts.append(ent.text)\n",
    "\n",
    "    return \" \".join(cleaned_parts)\n",
    "\n",
    "def latex_to_unicode(text):\n",
    "    \"\"\"Safely converts LaTeX special characters to Unicode, handling errors.\"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"\"  # Avoid empty input issues\n",
    "\n",
    "    try:\n",
    "        return converter.latex_to_text(text)\n",
    "    except IndexError as e:\n",
    "        print(f\"⚠️ Warning: Error parsing LaTeX in text: {text}\")\n",
    "        print(\"\\n⚠️ latex_to_unicode parse error encountered!\")\n",
    "        print(\"Full input string:\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(text)\n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(f\"Error message: {e}\\n\")\n",
    "        return text  # fallback: just return the original text\n",
    "\n",
    "def mild_clean_affiliation(affiliation):\n",
    "    parts = [p.strip() for p in affiliation.split(',')]\n",
    "    if len(parts) > 3:\n",
    "        for i, part in enumerate(parts):\n",
    "            if 'university' in part.lower():\n",
    "                if i < len(parts) - 1:\n",
    "                    return f\"{part}, {part}, {parts[i+1]}\"\n",
    "                else:\n",
    "                    return part\n",
    "        affiliation = ', '.join(parts[:3])\n",
    "    return affiliation.strip()\n",
    "\n",
    "\n",
    "def split_affiliations_before_unicode(raw_text):\n",
    "    temp_text = re.sub(r'(\\\\?\\\\?\\$\\\\?\\\\^)(\\\\?\\\\dag|\\\\?\\\\star|\\\\?\\*|\\\\?\\\\)', r'$^SPECIAL', raw_text)\n",
    "    split_result = re.split(r'\\\\?\\\\?\\$\\\\?\\\\^(?:SPECIAL|\\d+)', temp_text)\n",
    "    cleaned = [chunk.strip(' .\\\\') for chunk in split_result if chunk.strip()]\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def smart_pick_best_ror(query_text, candidates):\n",
    "    \"\"\"Choose the best matching ROR result based on exact match priority.\"\"\"\n",
    "    for idx, item in enumerate(candidates):\n",
    "        name = item.get('name', '').lower()\n",
    "        if name and name in query_text.lower():\n",
    "            print(f\"✅ Match found with candidate {idx+1}: {name}\")\n",
    "            return item\n",
    "        else:\n",
    "            print(f\"❌ No match for candidate {idx+1}: {name}\")\n",
    "    print(\"⚠️ No candidate matched exactly, fallback to first.\")\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "\n",
    "    \n",
    "stringa1 = r\"$^*$Physics Department, Arizona State University, Tempe, Arizona 85287, USA. \\\\ $^\\\\dag$Theoretical Physics Department, CERN, 1211 Geneva 23, Switzerland.\"\n",
    "stringa2 = r\"$^1$ Robotics and Artificial Intelligence Group, Department of Computer Science, Electrical and Space Engineering, Lule\\\\aa \\,\\, University of Technology, Sweden (e-mail: (vissan, sumsat, geonik)@ltu.se) \\\\ $^2$ Department of Computer Science, University of Helsinki, Finland.\"\n",
    "\n",
    "print(\"\\nOriginal input:\")\n",
    "print(stringa2)\n",
    "\n",
    "split_result = split_affiliations_before_unicode(stringa2)\n",
    "\n",
    "print(\"\\nSplit affiliations before unicode:\")\n",
    "for idx, aff in enumerate(split_result, 1):\n",
    "    print(f\"Institution {idx}: {aff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:🔍 Query: Instituto de Astrofísica e Ciências do Espaço, Universidade do Porto, CAUP\n",
      "INFO:root:❌ No match for candidate 1: Institute of Astrophysics and Space Sciences\n",
      "INFO:root:❌ No match for candidate 2: Instituto de Ciências da Terra e do Espaço\n",
      "INFO:root:❌ No match for candidate 3: Centre for Astrophysics of the University of Porto\n",
      "INFO:root:⚠️ No match found, fallback to first candidate: Institute of Astrophysics and Space Sciences\n",
      "INFO:root:🔍 Query: Instituto de Astrofísica de Canarias (IAC), E-38205 La Laguna, Tenerife\n",
      "INFO:root:✅ Match found with candidate 1: Instituto de Astrofísica de Canarias\n",
      "INFO:root:🔍 Query: Universidad de La Laguna (ULL), Departamento de Astrofísica, E-38206 La Laguna\n",
      "INFO:root:✅ Match found with candidate 1: Universidad de La Laguna\n",
      "INFO:root:🔍 Query: Ecole Centrale-Supelec, Université Paris-Saclay, 91190 Gif-sur-Yvette\n",
      "INFO:root:✅ Match found with candidate 1: Université Paris-Saclay\n",
      "INFO:root:🔍 Query: INAF – Osservatorio Astrofisico di Catania, Via S. Sofia, 78\n",
      "INFO:root:✅ Match found with candidate 1: Osservatorio Astrofisico di Catania\n",
      "ERROR:root:ROR API query failed with status code: 500 for institution: Institut für Physik, Karl-Franzens Universität Graz, Universitätsplatz 5/II\n",
      "INFO:root:🔍 Query: Instituto de Investigaciones en Energía no Convencional, Universidad Nacional de Salta, C.P.: 4400\n",
      "INFO:root:❌ No match for candidate 1: National University of Salta\n",
      "INFO:root:❌ No match for candidate 2: Instituto Nacional de Energia Elétrica\n",
      "INFO:root:❌ No match for candidate 3: Institut Català d'Energia\n",
      "INFO:root:⚠️ No match found, fallback to first candidate: National University of Salta\n",
      "INFO:root:🔍 Query: Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET), Godoy Cruz 2290, CABA\n",
      "INFO:root:✅ Match found with candidate 1: Consejo Nacional de Investigaciones Científicas y Técnicas\n",
      "INFO:root:🔍 Query: Departamento de Astronomía, Universidad de Chile, Casilla 36-D\n",
      "INFO:root:❌ No match for candidate 1: University of Chile\n",
      "INFO:root:❌ No match for candidate 2: Universidad de Santiago de Chile\n",
      "INFO:root:❌ No match for candidate 3: Bolivarian University of Chile\n",
      "INFO:root:⚠️ No match found, fallback to first candidate: University of Chile\n",
      "INFO:root:🔍 Query: University of California, University of California, San Diego\n",
      "INFO:root:✅ Match found with candidate 1: University of California, San Diego\n",
      "INFO:root:🔍 Query: University of Arizona, University of Arizona, 85719\n",
      "INFO:root:✅ Match found with candidate 1: University of Arizona\n",
      "INFO:root:🔍 Query: Joint ALMA Observatory (JAO), Alonso de Córdova 3107, Vitacura\n",
      "INFO:root:❌ No match for candidate 1: University of Córdoba\n",
      "INFO:root:❌ No match for candidate 2: African Leaders Malaria Alliance\n",
      "INFO:root:❌ No match for candidate 3: Alberta Livestock and Meat Agency\n",
      "INFO:root:⚠️ No match found, fallback to first candidate: University of Córdoba\n",
      "INFO:root:🔍 Query: Owens Valley Radio Observatory, California Institute of Technology, Big Pine\n",
      "INFO:root:❌ No match for candidate 1: Observatoire Radioastronomique de Nançay\n",
      "INFO:root:✅ Match found with candidate 2: California Institute of Technology\n",
      "INFO:root:🔍 Query: Texas A&M University, Texas A&M University, College Station\n",
      "INFO:root:✅ Match found with candidate 1: Texas A&M University\n",
      "INFO:root:🔍 Query: Chalmers University of Technology, Chalmers University of Technology, SE-439 92\n",
      "INFO:root:✅ Match found with candidate 1: Chalmers University of Technology\n",
      "INFO:pylatexenc.latexwalker:Ignoring parse error (tolerant parsing mode): End of input while parsing arguments of macro \"\\\" @(1,119)\n",
      "Open LaTeX blocks:\n",
      "          @(1,119)  arguments of macro \"\\\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Error parsing LaTeX in text: \\href{http://www.ncbj.gov.pl}{National Centre for Nuclear Research}\n",
      "\n",
      "⚠️ latex_to_unicode parse error encountered!\n",
      "Full input string:\n",
      "--------------------------------------------------\n",
      "\\href{http://www.ncbj.gov.pl}{National Centre for Nuclear Research}\n",
      "--------------------------------------------------\n",
      "Error message: list index out of range\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pylatexenc.latexwalker:Ignoring parse error (tolerant parsing mode): End of input while parsing arguments of macro \"\\\" @(1,93)\n",
      "Open LaTeX blocks:\n",
      "           @(1,93)  arguments of macro \"\\\"\n",
      "\n",
      "INFO:pylatexenc.latexwalker:Ignoring parse error (tolerant parsing mode): End of input while parsing arguments of macro \"\\\" @(1,93)\n",
      "Open LaTeX blocks:\n",
      "           @(1,93)  arguments of macro \"\\\"\n",
      "\n",
      "INFO:pylatexenc.latexwalker:Ignoring parse error (tolerant parsing mode): End of input while parsing arguments of macro \"\\\" @(1,82)\n",
      "Open LaTeX blocks:\n",
      "           @(1,82)  arguments of macro \"\\\"\n",
      "\n",
      "INFO:pylatexenc.latexwalker:Ignoring parse error (tolerant parsing mode): End of input while parsing arguments of macro \"\\\" @(1,123)\n",
      "Open LaTeX blocks:\n",
      "          @(1,123)  arguments of macro \"\\\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON export completed. Results saved to institution_output_with_ror.json\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "# Process each paper\n",
    "for paper, macros in extracted_data.items():\n",
    "    unique_rors = {}\n",
    "\n",
    "    if isinstance(macros, dict) and set(macros.keys()) in [{\"full_content\"}, {\"address\"}]:\n",
    "        text = \"\\n\".join(macros.get(\"full_content\", [])) + \"\\n\" + \"\\n\".join(macros.get(\"address\", []))\n",
    "        text = latex_to_unicode(text)\n",
    "        extracted_ror_ids = extractor.extract_affiliations(text)\n",
    "\n",
    "        if extracted_ror_ids:\n",
    "            unique_rors.update({ror_id: {\"ROR_ID\": ror_id} for ror_id in extracted_ror_ids})\n",
    "\n",
    "    else:\n",
    "        for content_list in macros.values():\n",
    "            for institution_name in content_list:\n",
    "                \n",
    "                raw_institutions = re.split(r'(?:\\\\\\\\)?\\\\and', institution_name)\n",
    "\n",
    "                for raw_inst in raw_institutions:\n",
    "                    raw_inst = raw_inst.strip()\n",
    "\n",
    "                    split_by_superscript = re.split(r'\\s*\\^\\d+', raw_inst)\n",
    "                    split_by_superscript = [inst.strip() for inst in split_by_superscript if inst.strip()]\n",
    "\n",
    "                    for raw_piece in split_by_superscript:\n",
    "                        if not raw_piece:\n",
    "                            continue\n",
    "\n",
    "                        cleaned_piece = latex_to_unicode(raw_piece)\n",
    "\n",
    "                        cleaned_piece = cleaned_piece.replace(\"{\", \" \").replace(\"}\", \" \").replace(\"^\", \" \").replace(\"[\", \" \").replace(\"]\", \" \").strip()\n",
    "                        cleaned_piece = mild_clean_affiliation(cleaned_piece)\n",
    "                        cleaned_piece = re.sub(r'^(.*?\\b(Department of|School of|Faculty of|College of|Department)\\b[^,]*),', '', cleaned_piece, flags=re.IGNORECASE)\n",
    "\n",
    "                        if cleaned_piece:\n",
    "                            ror_info = query_ror(cleaned_piece)\n",
    "                            if ror_info:\n",
    "                                unique_rors[ror_info[\"ROR_ID\"]] = ror_info  # Store unique RORs\n",
    "\n",
    "    if unique_rors:\n",
    "        results[paper] = list(unique_rors.values())\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(results, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ JSON export completed. Results saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Paper-level accuracy: 21.43% (12/56)\n",
      " Affiliation-level accuracy: 53.52% (114/213)\n",
      " Detailed evaluation saved to evaluation_details.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ground_truth = pd.read_csv(\"data/2311_with_ror.csv\")\n",
    "\n",
    "with open(\"institution_output_with_ror.json\", 'r', encoding='utf-8') as f:\n",
    "    extracted_results = json.load(f)\n",
    "\n",
    "ground_truth['ROR ID'] = ground_truth['ROR ID'].fillna('').astype(str)\n",
    "ground_truth['paper_id_clean'] = ground_truth['paper_id'].astype(str).str.strip().str.replace(r'v\\\\d+$', '', regex=True)\n",
    "\n",
    "gt_rors_per_paper = (\n",
    "    ground_truth.groupby('paper_id_clean')['ROR ID']\n",
    "    .apply(lambda x: set(ror.strip() for ror in x if ror.strip() != ''))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "correct_paper_level = 0\n",
    "total_papers = 0\n",
    "correct_affiliations = 0\n",
    "total_affiliations = 0\n",
    "evaluation_details = []\n",
    "\n",
    "for paper_id, extracted_rors_list in extracted_results.items():\n",
    "    paper_id_clean = paper_id.split('/')[-1]\n",
    "    extracted_rors = set(ror_info['ROR_ID'] for ror_info in extracted_rors_list)\n",
    "    gt_rors = gt_rors_per_paper.get(str(paper_id_clean), set())\n",
    "\n",
    "    paper_correct = extracted_rors == gt_rors\n",
    "    if paper_correct:\n",
    "        correct_paper_level += 1\n",
    "\n",
    "    total_papers += 1\n",
    "\n",
    "    matched_affiliations = extracted_rors.intersection(gt_rors)\n",
    "    missed_affiliations = gt_rors - extracted_rors\n",
    "    extra_affiliations = extracted_rors - gt_rors\n",
    "\n",
    "    for gt_ror in gt_rors:\n",
    "        total_affiliations += 1\n",
    "        if gt_ror in extracted_rors:\n",
    "            correct_affiliations += 1\n",
    "\n",
    "    evaluation_details.append({\n",
    "        \"paper_id\": paper_id_clean,\n",
    "        \"ground_truth_rors\": list(gt_rors),\n",
    "        \"extracted_rors\": list(extracted_rors),\n",
    "        \"matched_affiliations\": list(matched_affiliations),\n",
    "        \"missed_affiliations\": list(missed_affiliations),\n",
    "        \"extra_affiliations\": list(extra_affiliations),\n",
    "        \"paper_exact_match\": paper_correct\n",
    "    })\n",
    "\n",
    "paper_level_accuracy = correct_paper_level / total_papers if total_papers > 0 else 0\n",
    "affiliation_level_accuracy = correct_affiliations / total_affiliations if total_affiliations > 0 else 0\n",
    "\n",
    "# Save evaluation details to JSON\n",
    "with open(\"evaluation_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_details, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Paper-level accuracy: {paper_level_accuracy * 100:.2f}% ({correct_paper_level}/{total_papers})\")\n",
    "print(f\" Affiliation-level accuracy: {affiliation_level_accuracy * 100:.2f}% ({correct_affiliations}/{total_affiliations})\")\n",
    "print(\" Detailed evaluation saved to evaluation_details.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Query: Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801-3080, USA\n",
      "Number of results: 20\n",
      "Name: University of Illinois Urbana-Champaign\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/047426m28\n",
      "------\n",
      "Name: Urbana University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/04kp3hw27\n",
      "------\n",
      "Name: University of Illinois Chicago, Rockford campus\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02437s643\n",
      "------\n",
      "Name: University of Illinois Chicago\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02mpq6x41\n",
      "------\n",
      "Name: Peoria campus of the University of Illinois System\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02qrdc062\n",
      "------\n",
      "Name: University of Illinois at Springfield\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0126qma51\n",
      "------\n",
      "Name: Illinois Department of Agriculture\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/00pd5rv22\n",
      "------\n",
      "Name: Illinois Department of Transportation\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02vgzrk89\n",
      "------\n",
      "Name: University of Illinois System\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/05e94g991\n",
      "------\n",
      "Name: Illinois Department of Human Services\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/03gzw3461\n",
      "------\n",
      "Name: Illinois Department of Natural Resources\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/047g2hq96\n",
      "------\n",
      "Name: Illinois Department of Public Health\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/027bk5v43\n",
      "------\n",
      "Name: State of Illinois\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/025cbjw81\n",
      "------\n",
      "Name: The Urbana Free Library\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/04zd61g23\n",
      "------\n",
      "Name: Eastern Illinois University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/035z2ew45\n",
      "------\n",
      "Name: Northeastern Illinois University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/04edns687\n",
      "------\n",
      "Name: Illinois State University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/050kcr883\n",
      "------\n",
      "Name: Illinois Wesleyan University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/021aqk126\n",
      "------\n",
      "Name: Western Illinois University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02sshz518\n",
      "------\n",
      "Name: Northern Illinois University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/012wxa772\n",
      "------\n",
      "\n",
      "🔎 Query: Department of Radiology, Wake Forest University School of Medicine, Winston-Salem, North Carolina, USA\n",
      "Number of results: 20\n",
      "Name: Winston-Salem State University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/049yc0897\n",
      "------\n",
      "Name: Wake Forest University\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0207ad724\n",
      "------\n",
      "Name: Winston-Salem Chamber of Commerce\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/00y2td026\n",
      "------\n",
      "Name: North Carolina Institute of Medicine\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/042eghf65\n",
      "------\n",
      "Name: North Carolina Department of Administration\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/056w6an22\n",
      "------\n",
      "Name: North Carolina Department of Transportation\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02xmd8q53\n",
      "------\n",
      "Name: University of North Carolina School of the Arts\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/01vfsab41\n",
      "------\n",
      "Name: NC Department of Health and Human Services\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0293qmt12\n",
      "------\n",
      "Name: North Carolina Department of Public Safety\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02nnywp42\n",
      "------\n",
      "Name: North Carolina Department of Environmental Quality\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/01ftg8r37\n",
      "------\n",
      "Name: University of North Carolina Wilmington\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/02t0qr014\n",
      "------\n",
      "Name: University of North Carolina Hospitals\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0355zfr67\n",
      "------\n",
      "Name: University of North Carolina System\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0566a8c54\n",
      "------\n",
      "Name: Public School Forum of North Carolina\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/04dzh1r95\n",
      "------\n",
      "Name: Virginia Tech - Wake Forest University School of Biomedical Engineering & Sciences\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/01q1y4t48\n",
      "------\n",
      "Name: Winston-Salem/Forsyth County Schools\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/0340nty70\n",
      "------\n",
      "Name: North Carolina School of Science and Mathematics\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/03zbydc22\n",
      "------\n",
      "Name: University of North Carolina at Asheville\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/017c6at71\n",
      "------\n",
      "Name: University of North Carolina Health Care\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/00qz24g20\n",
      "------\n",
      "Name: University of North Carolina at Pembroke\n",
      "Score: None\n",
      "Country: United States\n",
      "ROR ID: https://ror.org/05pckj715\n",
      "------\n",
      "\n",
      "🔎 Query: Institute of Astronomy and Space Physics, Buenos Aires, Argentina\n",
      "Number of results: 20\n",
      "Name: Instituto de Física de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/00822yn93\n",
      "------\n",
      "Name: Institute of Astronomy and Space Physics\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/03rq94151\n",
      "------\n",
      "Name: Buenos Aires Institute of Technology\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/02qwadn23\n",
      "------\n",
      "Name: University of Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/0081fs513\n",
      "------\n",
      "Name: Astronomy and Space\n",
      "Score: None\n",
      "Country: Australia\n",
      "ROR ID: https://ror.org/04ynn1b95\n",
      "------\n",
      "Name: Hospital Británico de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/04djj4v98\n",
      "------\n",
      "Name: Observatorio Naval Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/01ae3s995\n",
      "------\n",
      "Name: Korea Astronomy and Space Science Institute\n",
      "Score: None\n",
      "Country: South Korea\n",
      "ROR ID: https://ror.org/04g2pxh42\n",
      "------\n",
      "Name: Buenos Aires Interdisciplinary Political Economy Institute\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/019ptgs60\n",
      "------\n",
      "Name: Academia Nacional de Medicina\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/05k2xsz75\n",
      "------\n",
      "Name: National University of Northwestern Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/00c8e6e47\n",
      "------\n",
      "Name: Universidad Nacional del Centro de la Provincia de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/011gakh74\n",
      "------\n",
      "Name: Pontificia Universidad Católica Argentina\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/0422kzb24\n",
      "------\n",
      "Name: Instituto Cardiovascular de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/05476d639\n",
      "------\n",
      "Name: Hospital Italiano de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/00bq4rw46\n",
      "------\n",
      "Name: Asociación Psicoanalítica de Buenos Aires\n",
      "Score: None\n",
      "Country: Argentina\n",
      "ROR ID: https://ror.org/03qtd7e32\n",
      "------\n",
      "Name: Swedish Institute of Space Physics\n",
      "Score: None\n",
      "Country: Sweden\n",
      "ROR ID: https://ror.org/043kppn11\n",
      "------\n",
      "Name: HUN-REN Institute of Earth Physics and Space Science\n",
      "Score: None\n",
      "Country: Hungary\n",
      "ROR ID: https://ror.org/05c9vr219\n",
      "------\n",
      "Name: Institute of Astronomy\n",
      "Score: None\n",
      "Country: Russia\n",
      "ROR ID: https://ror.org/01whksa54\n",
      "------\n",
      "Name: Purple Mountain Observatory\n",
      "Score: None\n",
      "Country: China\n",
      "ROR ID: https://ror.org/02eb4t121\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_ror_query(institution_name):\n",
    "    ROR_SEARCH_URL = 'https://api.ror.org/organizations'\n",
    "    params = {'query': institution_name}\n",
    "    \n",
    "    response = requests.get(ROR_SEARCH_URL, params=params, timeout=10)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(f\"\\n🔎 Query: {institution_name}\")\n",
    "        print(f\"Number of results: {len(data.get('items', []))}\")\n",
    "        for item in data.get('items', []):\n",
    "            print(f\"Name: {item.get('name')}\")\n",
    "            print(f\"Score: {item.get('score')}\")\n",
    "            print(f\"Country: {item.get('country', {}).get('country_name')}\")\n",
    "            print(f\"ROR ID: {item.get('id')}\")\n",
    "            print(\"------\")\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\")\n",
    "\n",
    "# Example test cases:\n",
    "test_ror_query(\"Department of Physics, University of Illinois at Urbana-Champaign, Urbana, Illinois 61801-3080, USA\")\n",
    "test_ror_query(\"Department of Radiology, Wake Forest University School of Medicine, Winston-Salem, North Carolina, USA\")\n",
    "test_ror_query(\"Institute of Astronomy and Space Physics, Buenos Aires, Argentina\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('ror/v1.52-2024-09-16-ror-data_schema_v2.json', 'r', encoding='utf-8') as f:\n",
    "    ror_data = json.load(f)\n",
    "\n",
    "ror_relationships = {}\n",
    "\n",
    "for record in ror_data:\n",
    "    ror_id = record['id']\n",
    "    related_ids = set()\n",
    "    for rel in record.get('relationships', []):\n",
    "        related_ids.add(rel['id'])\n",
    "    ror_relationships[ror_id] = related_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_match(ror1, ror2, ror_relationships):\n",
    "    related1 = ror_relationships.get(ror1, set())\n",
    "    related2 = ror_relationships.get(ror2, set())\n",
    "    return (ror1 == ror2) or (ror2 in related1) or (ror1 in related2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Paper-level accuracy: 39.29% (22/56)\n",
      " Affiliation-level accuracy: 75.12% (160/213)\n",
      " Detailed evaluation saved to evaluation_details.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "ground_truth = pd.read_csv(\"data/2311_with_ror.csv\")\n",
    "\n",
    "with open(\"institution_output_with_ror.json\", 'r', encoding='utf-8') as f:\n",
    "    extracted_results = json.load(f)\n",
    "\n",
    "ground_truth['ROR ID'] = ground_truth['ROR ID'].fillna('').astype(str)\n",
    "ground_truth['paper_id_clean'] = ground_truth['paper_id'].astype(str).str.strip().str.replace(r'v\\\\d+$', '', regex=True)\n",
    "\n",
    "gt_rors_per_paper = (\n",
    "    ground_truth.groupby('paper_id_clean')['ROR ID']\n",
    "    .apply(lambda x: set(ror.strip() for ror in x if ror.strip() != ''))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "correct_paper_level = 0\n",
    "total_papers = 0\n",
    "correct_affiliations = 0\n",
    "total_affiliations = 0\n",
    "evaluation_details = []\n",
    "\n",
    "for paper_id, extracted_rors_list in extracted_results.items():\n",
    "    paper_id_clean = paper_id.split('/')[-1]\n",
    "    extracted_rors = set(ror_info['ROR_ID'] for ror_info in extracted_rors_list)\n",
    "    gt_rors = gt_rors_per_paper.get(str(paper_id_clean), set())\n",
    "\n",
    "    paper_correct = all(\n",
    "        any(is_match(gt_ror, extracted_ror, ror_relationships) for extracted_ror in extracted_rors)\n",
    "        for gt_ror in gt_rors\n",
    "    ) and all(\n",
    "        any(is_match(extracted_ror, gt_ror, ror_relationships) for gt_ror in gt_rors)\n",
    "        for extracted_ror in extracted_rors\n",
    "    )\n",
    "\n",
    "    if paper_correct:\n",
    "        correct_paper_level += 1\n",
    "    total_papers += 1\n",
    "\n",
    "    real_missed = []\n",
    "    for gt_ror in gt_rors:\n",
    "        total_affiliations += 1\n",
    "        if any(is_match(gt_ror, extracted_ror, ror_relationships) for extracted_ror in extracted_rors):\n",
    "            correct_affiliations += 1\n",
    "        else:\n",
    "            real_missed.append(gt_ror)\n",
    "\n",
    "    real_extra = []\n",
    "    for ext_ror in extracted_rors:\n",
    "        if not any(is_match(ext_ror, gt_ror, ror_relationships) for gt_ror in gt_rors):\n",
    "            real_extra.append(ext_ror)\n",
    "\n",
    "    evaluation_details.append({\n",
    "        \"paper_id\": paper_id_clean,\n",
    "        \"ground_truth_rors\": list(gt_rors),\n",
    "        \"extracted_rors\": list(extracted_rors),\n",
    "        \"matched_affiliations\": list(gt_rors.intersection(extracted_rors)),  # optional\n",
    "        \"missed_affiliations\": real_missed,\n",
    "        \"extra_affiliations\": real_extra,\n",
    "        \"paper_exact_match\": paper_correct\n",
    "    })\n",
    "\n",
    "paper_level_accuracy = correct_paper_level / total_papers if total_papers > 0 else 0\n",
    "affiliation_level_accuracy = correct_affiliations / total_affiliations if total_affiliations > 0 else 0\n",
    "\n",
    "with open(\"evaluation_details_relationship.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_details, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n Paper-level accuracy: {paper_level_accuracy * 100:.2f}% ({correct_paper_level}/{total_papers})\")\n",
    "print(f\" Affiliation-level accuracy: {affiliation_level_accuracy * 100:.2f}% ({correct_affiliations}/{total_affiliations})\")\n",
    "print(\" Detailed evaluation saved to evaluation_details.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Paper-level accuracy: 41.25% (825/2000)\n",
      "✅ Affiliation-level accuracy: 57.71% (3965/6870)\n",
      "✅ Detailed evaluation saved to evaluation_details_relationship.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load ground truth\n",
    "ground_truth = pd.read_csv(\"data/2311_with_ror.csv\")\n",
    "\n",
    "# Step 2: Load extracted results from new file\n",
    "with open(\"final_affiliations_2000_parallel.json\", 'r', encoding='utf-8') as f:\n",
    "    extracted_results_raw = json.load(f)\n",
    "\n",
    "# Step 3: Preprocess ground truth\n",
    "ground_truth['ROR ID'] = ground_truth['ROR ID'].fillna('').astype(str)\n",
    "ground_truth['paper_id_clean'] = ground_truth['paper_id'].astype(str).str.strip().str.replace(r'v\\\\d+$', '', regex=True)\n",
    "\n",
    "gt_rors_per_paper = (\n",
    "    ground_truth.groupby('paper_id_clean')['ROR ID']\n",
    "    .apply(lambda x: set(ror.strip() for ror in x if ror.strip() != ''))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "# Step 4: Preprocess extracted results into a {paper_id: set(rors)} format\n",
    "extracted_results = {}\n",
    "for item in extracted_results_raw:\n",
    "    paper_id = item.get(\"File ID\", \"\").strip()\n",
    "    extracted_rors = set()\n",
    "    for inst in item.get(\"institutions_with_ror\", []):\n",
    "        ror_id = inst.get(\"ror_id\", \"\").strip()\n",
    "        if ror_id:\n",
    "            # Add full URL form if needed\n",
    "            if not ror_id.startswith(\"https://ror.org/\"):\n",
    "                ror_id = f\"https://ror.org/{ror_id}\"\n",
    "            extracted_rors.add(ror_id)\n",
    "    extracted_results[paper_id] = extracted_rors\n",
    "\n",
    "# Step 5: Load ROR relationships file\n",
    "with open(\"ror/v1.52-2024-09-16-ror-data_schema_v2.json\", 'r', encoding='utf-8') as f:\n",
    "    ror_full_data = json.load(f)\n",
    "\n",
    "# Build relationship dictionary\n",
    "ror_relationships = {}\n",
    "for entry in ror_full_data:\n",
    "    ror_id = entry.get(\"id\", \"\")\n",
    "    if ror_id:\n",
    "        children = set(rel[\"id\"] for rel in entry.get(\"relationships\", []) if rel[\"type\"] == \"child\")\n",
    "        parents = set(rel[\"id\"] for rel in entry.get(\"relationships\", []) if rel[\"type\"] == \"parent\")\n",
    "        ror_relationships[ror_id] = {\"children\": children, \"parents\": parents}\n",
    "\n",
    "# Step 6: Define is_match function\n",
    "def is_match(ror_a, ror_b, relationships):\n",
    "    if ror_a == ror_b:\n",
    "        return True\n",
    "    a_rel = relationships.get(ror_a, {\"children\": set(), \"parents\": set()})\n",
    "    b_rel = relationships.get(ror_b, {\"children\": set(), \"parents\": set()})\n",
    "    if ror_b in a_rel[\"children\"] or ror_b in a_rel[\"parents\"]:\n",
    "        return True\n",
    "    if ror_a in b_rel[\"children\"] or ror_a in b_rel[\"parents\"]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Step 7: Evaluate\n",
    "correct_paper_level = 0\n",
    "total_papers = 0\n",
    "correct_affiliations = 0\n",
    "total_affiliations = 0\n",
    "evaluation_details = []\n",
    "\n",
    "for paper_id, extracted_rors in extracted_results.items():\n",
    "    gt_rors = gt_rors_per_paper.get(paper_id, set())\n",
    "\n",
    "    paper_correct = all(\n",
    "        any(is_match(gt_ror, extracted_ror, ror_relationships) for extracted_ror in extracted_rors)\n",
    "        for gt_ror in gt_rors\n",
    "    ) and all(\n",
    "        any(is_match(extracted_ror, gt_ror, ror_relationships) for gt_ror in gt_rors)\n",
    "        for extracted_ror in extracted_rors\n",
    "    )\n",
    "\n",
    "    if paper_correct:\n",
    "        correct_paper_level += 1\n",
    "    total_papers += 1\n",
    "\n",
    "    real_missed = []\n",
    "    for gt_ror in gt_rors:\n",
    "        total_affiliations += 1\n",
    "        if any(is_match(gt_ror, extracted_ror, ror_relationships) for extracted_ror in extracted_rors):\n",
    "            correct_affiliations += 1\n",
    "        else:\n",
    "            real_missed.append(gt_ror)\n",
    "\n",
    "    real_extra = []\n",
    "    for ext_ror in extracted_rors:\n",
    "        if not any(is_match(ext_ror, gt_ror, ror_relationships) for gt_ror in gt_rors):\n",
    "            real_extra.append(ext_ror)\n",
    "\n",
    "    evaluation_details.append({\n",
    "        \"paper_id\": paper_id,\n",
    "        \"ground_truth_rors\": list(gt_rors),\n",
    "        \"extracted_rors\": list(extracted_rors),\n",
    "        \"matched_affiliations\": list(gt_rors.intersection(extracted_rors)),\n",
    "        \"missed_affiliations\": real_missed,\n",
    "        \"extra_affiliations\": real_extra,\n",
    "        \"paper_exact_match\": paper_correct\n",
    "    })\n",
    "\n",
    "paper_level_accuracy = correct_paper_level / total_papers if total_papers > 0 else 0\n",
    "affiliation_level_accuracy = correct_affiliations / total_affiliations if total_affiliations > 0 else 0\n",
    "\n",
    "# Step 8: Save evaluation results\n",
    "with open(\"evaluation_details_relationship.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_details, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\n✅ Paper-level accuracy: {paper_level_accuracy * 100:.2f}% ({correct_paper_level}/{total_papers})\")\n",
    "print(f\"✅ Affiliation-level accuracy: {affiliation_level_accuracy * 100:.2f}% ({correct_affiliations}/{total_affiliations})\")\n",
    "print(\"✅ Detailed evaluation saved to evaluation_details_relationship.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
